{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Methods - Data Analysis\n",
    "_____\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Libraries](#libraries)\n",
    "2. [Data Preprocessing](#data-preprocessing)\n",
    "3. [Topic Modelling](#topic-modelling)\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "All libraries which are needed to execute the code are listed here. Install the packages by using the `requirements.txt` file. \n",
    "\n",
    "The documentation can be found in the [README.md](README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "import os\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel\n",
    "\n",
    "# import functions\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "df = pd.read_csv('comments.csv', index_col=0)\n",
    "\n",
    "#set pandas option to show more text\n",
    "pd.set_option('display.max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data with using functions from functions.py\n",
    "processed_df = (\n",
    "    df.pipe(remove_users, 'text')\n",
    "      .pipe(lowercase_text, 'text')\n",
    "      .pipe(remove_numbers, 'text')\n",
    "      .pipe(remove_whitespace, 'text')\n",
    "      .pipe(remove_stopwords, 'text')\n",
    "      .pipe(remove_punctuation, 'text')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text column to string\n",
    "processed_df['text'] = processed_df['text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stemming to reduce words to their root words\n",
    "processed_df = stem_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lemmatization to reduce words to their root form\n",
    "processed_df = lemmatize_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date format\n",
    "processed_df = convert_date_format(processed_df, 'published_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN-values and aggregating data by date\n",
    "processed_df.lemmatized_text = processed_df.lemmatized_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "processed_df.stemmed_text = processed_df.stemmed_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "\n",
    "\n",
    "# aggregating tweet data by dates and affiliation \n",
    "df_agg = processed_df.groupby(['published_at', 'video_id'], as_index = False).agg({'text': ' '.join, \n",
    "                                                                            'lemmatized_text': ' '.join,\n",
    "                                                                            'stemmed_text': ' '.join})\n",
    "# checking dimensions of new dataset and viewing the dataset\n",
    "print(df_agg.shape)\n",
    "\n",
    "#Defining NLTK's TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# tokenizing and creating a column of unigrams from the stemmed tweet text. \n",
    "df_agg['unigrams'] = df_agg['stemmed_text'].progress_apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a column with bigrams by applying function to column of unigrams\n",
    "df_agg['bigrams'] = df_agg.unigrams.progress_apply(lambda x: bigrams(x))\n",
    "df_agg['tokens'] = df_agg.unigrams+df_agg.bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the column where you saved unigram and bigram tokens between the parentheses\n",
    "id2word = Dictionary(df_agg['tokens']) \n",
    "\n",
    "# viewing how many words are in our vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing very frequent and infrequent words\n",
    "id2word.filter_extremes(no_below=10, \n",
    "                        no_above=.999,\n",
    "                        keep_n=None) \n",
    "\n",
    "# viewing how many words are now in our vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating corpus\n",
    "corpus = [id2word.doc2bow(doc) for doc in df_agg['tokens']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "___\n",
    "\n",
    "\n",
    "- using topic modelling to explore for keywords.\n",
    "- using LSI, HDP, and LDA to get an impression on the topics of our observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating LSI Model\n",
    "lsi_model = LsiModel(corpus=corpus, num_topics=10, id2word=id2word)\n",
    "lsi_model.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating HDP Model\n",
    "hdp_model = HdpModel(corpus=corpus, id2word=id2word)\n",
    "hdp_model.show_topics()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LDA Model\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=10, id2word=id2word)\n",
    "lda_model.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

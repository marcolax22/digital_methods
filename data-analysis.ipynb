{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Methods - Data Analysis\n",
    "_____\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Libraries](#libraries)\n",
    "2. [Data Preprocessing](#data-preprocessing)\n",
    "3. [Topic Modelling](#topic-modelling)\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "All libraries which are needed to execute the code are listed here. Install the packages by using the `requirements.txt` file. \n",
    "\n",
    "The documentation can be found in the [README.md](README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tqdm import tqdm \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, LsiModel, HdpModel\n",
    "\n",
    "# import functions\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "df = pd.read_csv('comments.csv', index_col=0)\n",
    "\n",
    "#set pandas option to show more text\n",
    "pd.set_option('display.max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data with using functions from functions.py\n",
    "processed_df = (\n",
    "    df.pipe(remove_users, 'text')\n",
    "      .pipe(lowercase_text, 'text')\n",
    "      .pipe(remove_numbers, 'text')\n",
    "      .pipe(remove_whitespace, 'text')\n",
    "      .pipe(remove_stopwords, 'text')\n",
    "      .pipe(remove_punctuation, 'text')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text column to string\n",
    "processed_df['text'] = processed_df['text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stemming to reduce words to their root words\n",
    "processed_df = stem_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lemmatization to reduce words to their root form\n",
    "processed_df = lemmatize_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date format\n",
    "processed_df = convert_date_format(processed_df, 'published_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7333, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7333/7333 [00:05<00:00, 1333.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Replacing NaN-values and aggregating data by date\n",
    "processed_df.lemmatized_text = processed_df.lemmatized_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "processed_df.stemmed_text = processed_df.stemmed_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "\n",
    "\n",
    "# aggregating tweet data by dates and affiliation \n",
    "df_agg = processed_df.groupby(['published_at', 'video_id'], as_index = False).agg({'text': ' '.join, \n",
    "                                                                            'lemmatized_text': ' '.join,\n",
    "                                                                            'stemmed_text': ' '.join})\n",
    "# checking dimensions of new dataset and viewing the dataset\n",
    "print(df_agg.shape)\n",
    "\n",
    "#Defining NLTK's TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# tokenizing and creating a column of unigrams from the stemmed tweet text. \n",
    "df_agg['unigrams'] = df_agg['stemmed_text'].progress_apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7333/7333 [00:00<00:00, 16481.80it/s]\n"
     ]
    }
   ],
   "source": [
    "#Creating a column with bigrams by applying function to column of unigrams\n",
    "df_agg['bigrams'] = df_agg.unigrams.progress_apply(lambda x: bigrams(x))\n",
    "df_agg['tokens'] = df_agg.unigrams+df_agg.bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896734\n"
     ]
    }
   ],
   "source": [
    "# insert the column where you saved unigram and bigram tokens between the parentheses\n",
    "id2word = Dictionary(df_agg['tokens']) \n",
    "\n",
    "# viewing how many words are in our vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19198\n"
     ]
    }
   ],
   "source": [
    "# removing very frequent and infrequent words\n",
    "id2word.filter_extremes(no_below=10, \n",
    "                        no_above=.999,\n",
    "                        keep_n=None) \n",
    "\n",
    "# viewing how many words are now in our vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating corpus\n",
    "corpus = [id2word.doc2bow(doc) for doc in df_agg['tokens']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "___\n",
    "\n",
    "\n",
    "- using topic modelling to explore for keywords.\n",
    "- using LSI, HDP, and LDA to get an impression on the topics of our observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.387*\"peopl\" + 0.306*\"â€™\" + 0.229*\"govern\" + 0.184*\"land\" + 0.165*\"fire\" + 0.162*\"like\" + 0.155*\"get\" + 0.128*\"go\" + 0.122*\"climat\" + 0.117*\"need\"'),\n",
       " (1,\n",
       "  '0.507*\"climat\" + 0.341*\"chang\" + 0.264*\"climat_chang\" + -0.187*\"land\" + -0.174*\"govern\" + -0.157*\"fire\" + -0.143*\"peopl\" + 0.132*\"year\" + -0.126*\"maui\" + -0.124*\"tulsi\"'),\n",
       " (2,\n",
       "  '0.289*\"kerri\" + -0.260*\"chang\" + 0.222*\"ðŸ˜‚\" + -0.210*\"climat\" + 0.209*\"fuel\" + -0.197*\"climat_chang\" + 0.176*\"fossil\" + 0.169*\"lie\" + 0.159*\"fossil_fuel\" + 0.152*\"like\"'),\n",
       " (3,\n",
       "  '0.427*\"kerri\" + -0.304*\"fuel\" + -0.279*\"fossil\" + -0.252*\"fossil_fuel\" + 0.211*\"john\" + 0.172*\"lie\" + 0.171*\"john_kerri\" + 0.159*\"fli\" + -0.152*\"oil\" + -0.136*\"energi\"'),\n",
       " (4,\n",
       "  '-0.340*\"ðŸ˜‚\" + -0.266*\"expert\" + -0.203*\"kid\" + -0.190*\"chang\" + 0.169*\"kerri\" + 0.159*\"thank\" + -0.156*\"ðŸ˜‚_ðŸ˜‚\" + 0.153*\"toni\" + -0.142*\"greta\" + -0.140*\"climat_chang\"'),\n",
       " (5,\n",
       "  '0.263*\"greta\" + 0.202*\"expert\" + 0.185*\"know\" + 0.181*\"co\" + -0.171*\"climat_chang\" + -0.170*\"chang\" + -0.165*\"climat\" + 0.162*\"toni\" + -0.140*\"fuel\" + 0.128*\"thank\"'),\n",
       " (6,\n",
       "  '0.347*\"cloud\" + 0.319*\"seed\" + 0.258*\"cloud_seed\" + 0.211*\"weather\" + 0.150*\"rain\" + -0.135*\"expert\" + -0.116*\"money\" + -0.114*\"chang\" + 0.112*\"sun\" + 0.111*\"year\"'),\n",
       " (7,\n",
       "  '0.518*\"greta\" + -0.218*\"co\" + -0.166*\"expert\" + -0.144*\"toni\" + -0.130*\"year\" + 0.123*\"russia\" + 0.119*\"climat_chang\" + -0.119*\"temperatur\" + -0.116*\"carbon\" + 0.113*\"nuclear\"'),\n",
       " (8,\n",
       "  '0.361*\"greta\" + 0.283*\"toni\" + -0.280*\"cloud\" + -0.273*\"seed\" + -0.220*\"cloud_seed\" + -0.166*\"â€™\" + 0.126*\"lie\" + 0.116*\"thank\" + -0.111*\"peterson\" + -0.109*\"dr\"'),\n",
       " (9,\n",
       "  '0.479*\"toni\" + -0.171*\"greta\" + 0.161*\"thank\" + -0.157*\"co\" + -0.149*\"earth\" + 0.147*\"climat\" + 0.140*\"thank_toni\" + -0.130*\"sun\" + -0.128*\"wef\" + 0.126*\"seed\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating LSI Model\n",
    "lsi_model = LsiModel(corpus=corpus, num_topics=10, id2word=id2word)\n",
    "lsi_model.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*climat + 0.008*â€™ + 0.008*peopl + 0.007*chang + 0.005*like + 0.005*climat_chang + 0.005*year + 0.004*get + 0.004*one + 0.004*would + 0.004*go + 0.004*know + 0.003*us + 0.003*co + 0.003*need + 0.003*say + 0.003*time + 0.003*make + 0.003*use + 0.003*world'),\n",
       " (1,\n",
       "  '0.011*climat + 0.006*chang + 0.005*â€™ + 0.005*peopl + 0.004*climat_chang + 0.004*like + 0.004*year + 0.004*would + 0.004*one + 0.003*scienc + 0.003*co + 0.003*thank + 0.003*get + 0.003*use + 0.003*time + 0.003*know + 0.003*scientist + 0.003*earth + 0.003*say + 0.003*warm'),\n",
       " (2,\n",
       "  '0.006*climat + 0.004*peopl + 0.003*chang + 0.003*â€™ + 0.003*energi + 0.003*like + 0.003*use + 0.002*climat_chang + 0.002*one + 0.002*world + 0.002*year + 0.002*would + 0.002*need + 0.002*get + 0.002*make + 0.002*dr + 0.002*know + 0.002*us + 0.002*power + 0.002*thank'),\n",
       " (3,\n",
       "  '0.007*climat + 0.004*chang + 0.003*peopl + 0.003*climat_chang + 0.003*co + 0.003*scienc + 0.003*year + 0.003*global + 0.003*like + 0.002*say + 0.002*thank + 0.002*â€™ + 0.002*warm + 0.002*dr + 0.002*one + 0.002*world + 0.002*think + 0.002*earth + 0.002*would + 0.002*scientist'),\n",
       " (4,\n",
       "  '0.005*climat + 0.004*chang + 0.003*like + 0.003*â€™ + 0.003*climat_chang + 0.003*peopl + 0.002*would + 0.002*get + 0.002*look + 0.002*go + 0.002*one + 0.002*world + 0.002*see + 0.002*greta + 0.002*us + 0.002*year + 0.002*thank + 0.002*want + 0.002*dr + 0.002*stage'),\n",
       " (5,\n",
       "  '0.005*climat + 0.004*â€™ + 0.003*year + 0.003*like + 0.003*toni + 0.003*go + 0.002*peopl + 0.002*chang + 0.002*normal + 0.002*back + 0.002*would + 0.002*power + 0.002*warm + 0.002*world + 0.002*wind + 0.002*greta + 0.002*energi + 0.002*climat_chang + 0.002*time + 0.002*never'),\n",
       " (6,\n",
       "  '0.006*climat + 0.003*chang + 0.003*peopl + 0.002*thank + 0.002*climat_chang + 0.002*like + 0.002*co + 0.002*â€™ + 0.002*get + 0.002*dr + 0.002*one + 0.002*lindzen + 0.002*go + 0.002*make + 0.002*would + 0.002*year + 0.002*scientist + 0.002*good + 0.001*scienc + 0.001*us'),\n",
       " (7,\n",
       "  '0.004*climat + 0.003*lindzen + 0.002*chang + 0.002*â€™ + 0.002*scienc + 0.002*model + 0.002*climat_chang + 0.002*thank + 0.002*scientist + 0.002*would + 0.002*dr + 0.002*one + 0.002*much + 0.002*peopl + 0.002*get + 0.001*world + 0.001*warm + 0.001*think + 0.001*jordan + 0.001*like'),\n",
       " (8,\n",
       "  '0.005*climat + 0.003*greta + 0.003*chang + 0.002*like + 0.002*â€™ + 0.002*get + 0.002*one + 0.002*anxieti + 0.002*climat_chang + 0.002*peopl + 0.002*child + 0.002*parent + 0.001*world + 0.001*go + 0.001*make + 0.001*year + 0.001*say + 0.001*start + 0.001*stop + 0.001*look'),\n",
       " (9,\n",
       "  '0.005*climat + 0.003*chang + 0.003*climat_chang + 0.002*year + 0.002*like + 0.002*thank + 0.002*â€™ + 0.001*would + 0.001*co + 0.001*scienc + 0.001*scientist + 0.001*make + 0.001*warm + 0.001*dr + 0.001*peopl + 0.001*global + 0.001*real + 0.001*great + 0.001*lindzen + 0.001*us')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating HDP Model\n",
    "hdp_model = HdpModel(corpus=corpus, id2word=id2word)\n",
    "hdp_model.show_topics()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"â€™\" + 0.009*\"climat\" + 0.006*\"know\" + 0.006*\"peopl\" + 0.005*\"like\" + 0.005*\"chang\" + 0.005*\"would\" + 0.005*\"year\" + 0.004*\"ðŸ˜‚\" + 0.004*\"climat_chang\"'),\n",
       " (1,\n",
       "  '0.008*\"energi\" + 0.007*\"co\" + 0.007*\"climat\" + 0.005*\"use\" + 0.004*\"would\" + 0.004*\"temperatur\" + 0.004*\"tom\" + 0.004*\"thank\" + 0.004*\"chang\" + 0.004*\"atmospher\"'),\n",
       " (2,\n",
       "  '0.034*\"fossil\" + 0.028*\"fuel\" + 0.025*\"fossil_fuel\" + 0.014*\"oil\" + 0.008*\"climat\" + 0.006*\"energi\" + 0.006*\"use\" + 0.005*\"â€™\" + 0.004*\"would\" + 0.004*\"like\"'),\n",
       " (3,\n",
       "  '0.015*\"climat\" + 0.010*\"chang\" + 0.008*\"climat_chang\" + 0.007*\"peopl\" + 0.007*\"year\" + 0.006*\"â€™\" + 0.005*\"like\" + 0.005*\"get\" + 0.005*\"toni\" + 0.004*\"know\"'),\n",
       " (4,\n",
       "  '0.014*\"climat\" + 0.009*\"â€™\" + 0.007*\"chang\" + 0.007*\"peopl\" + 0.007*\"year\" + 0.005*\"climat_chang\" + 0.005*\"scienc\" + 0.005*\"thank\" + 0.004*\"govern\" + 0.004*\"get\"'),\n",
       " (5,\n",
       "  '0.016*\"climat\" + 0.008*\"co\" + 0.008*\"chang\" + 0.007*\"year\" + 0.006*\"thank\" + 0.005*\"â€™\" + 0.005*\"like\" + 0.005*\"climat_chang\" + 0.004*\"earth\" + 0.004*\"ice\"'),\n",
       " (6,\n",
       "  '0.011*\"climat\" + 0.011*\"peopl\" + 0.010*\"chang\" + 0.007*\"â€™\" + 0.006*\"climat_chang\" + 0.006*\"year\" + 0.005*\"like\" + 0.005*\"go\" + 0.004*\"need\" + 0.004*\"would\"'),\n",
       " (7,\n",
       "  '0.009*\"â€™\" + 0.008*\"climat\" + 0.008*\"peopl\" + 0.008*\"like\" + 0.007*\"chang\" + 0.005*\"climat_chang\" + 0.005*\"know\" + 0.004*\"get\" + 0.004*\"one\" + 0.004*\"make\"'),\n",
       " (8,\n",
       "  '0.014*\"climat\" + 0.012*\"â€™\" + 0.010*\"peopl\" + 0.007*\"chang\" + 0.007*\"go\" + 0.005*\"us\" + 0.005*\"get\" + 0.005*\"like\" + 0.005*\"climat_chang\" + 0.004*\"say\"'),\n",
       " (9,\n",
       "  '0.013*\"peopl\" + 0.011*\"â€™\" + 0.005*\"govern\" + 0.005*\"get\" + 0.005*\"fire\" + 0.005*\"vote\" + 0.005*\"like\" + 0.005*\"go\" + 0.005*\"money\" + 0.005*\"land\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating LDA Model\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=10, id2word=id2word)\n",
    "lda_model.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

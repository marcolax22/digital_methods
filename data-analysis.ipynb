{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Methods - Data Analysis\n",
    "_____\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Libraries](#libraries)\n",
    "2. [Data Preprocessing](#data-preprocessing)\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "All libraries which are needed to execute the code are listed here. Install the packages by using the `requirements.txt` file. \n",
    "\n",
    "The documentation can be found in the [README.md](README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel\n",
    "\n",
    "from preprocessing_function import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir('/Users/marco/Documents/Master Social Data Science/Semester 2/Digital Methods/data') \n",
    "df = pd.read_csv('comments.csv', index_col=0)\n",
    "\n",
    "#set pandas option to show more text\n",
    "pd.set_option('display.max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data with using functions from functions.py\n",
    "processed_df = (\n",
    "    df.pipe(remove_users, 'text')\n",
    "      .pipe(lowercase_text, 'text')\n",
    "      .pipe(remove_numbers, 'text')\n",
    "      .pipe(remove_whitespace, 'text')\n",
    "      .pipe(remove_stopwords, 'text')\n",
    "      .pipe(remove_punctuation, 'text')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id        object\n",
       "published_at    object\n",
       "like_count       int64\n",
       "text            object\n",
       "author          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['text'] = processed_df['text'].astype('str')\n",
    "processed_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stemming to reduce words to their root words\n",
    "processed_df = stem_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lemmatization to reduce words to their root form\n",
    "processed_df = lemmatize_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv(\"processed_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv('processed_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>1</td>\n",
       "      <td>answer   china  india  help   matter  much money  rest   world throws  reducing carbon footprint  complete waste  trillion dollars ü§¶‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>@ilandgrl</td>\n",
       "      <td>answer china india help matter much money rest world throw reduc carbon footprint complet wast trillion dollar ü§¶‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>answer china india help matter much money rest world throw reduce carbon footprint complete waste trillion dollar ü§¶‚Äç‚ôÄÔ∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>2</td>\n",
       "      <td>guy   expert  screwed</td>\n",
       "      <td>@lobusdiMortis</td>\n",
       "      <td>guy expert screw</td>\n",
       "      <td>guy expert screw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>4</td>\n",
       "      <td>kennedy   gem</td>\n",
       "      <td>@skitz1337</td>\n",
       "      <td>kennedi gem</td>\n",
       "      <td>kennedy gem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>0</td>\n",
       "      <td>get  nation like china  cooperate  even trust    telling  truth   say    absolute ridiculousness   occurring  western nations supposedly   advanced  capable   leads  science  technology  willing  cripple  impoverish millions   sake  iffy pseudo science     experts care anyway  wealthy elitists    least affected   ramifications   extreme measures   would promote   rest   world</td>\n",
       "      <td>@dodieodie498</td>\n",
       "      <td>get nation like china cooper even trust tell truth say absolut ridicul occur western nation suppos advanc capabl lead scienc technolog will crippl impoverish million sake iffi pseudo scienc expert care anyway wealthi elitist least affect ramif extrem measur would promot rest world</td>\n",
       "      <td>get nation like china cooperate even trust tell truth say absolute ridiculousness occur western nation supposedly advance capable lead science technology willing cripple impoverish million sake iffy pseudo science expert care anyway wealthy elitist least affected ramification extreme measure would promote rest world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>man  going   oscar</td>\n",
       "      <td>@karenh4458</td>\n",
       "      <td>man go oscar</td>\n",
       "      <td>man go oscar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96590</th>\n",
       "      <td>eAnVFKndFoY</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>0</td>\n",
       "      <td>course ‚Äô theatre global warming   hoax  use  fear  control low iq individuals</td>\n",
       "      <td>@internetexplorer6097</td>\n",
       "      <td>cours ‚Äô theatr global warm hoax use fear control low iq individu</td>\n",
       "      <td>course ‚Äô theatre global warm hoax use fear control low iq individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96591</th>\n",
       "      <td>eAnVFKndFoY</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>4</td>\n",
       "      <td>yup</td>\n",
       "      <td>@bonitabeach3127</td>\n",
       "      <td>yup</td>\n",
       "      <td>yup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96592</th>\n",
       "      <td>eAnVFKndFoY</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>5</td>\n",
       "      <td>staged</td>\n",
       "      <td>@kevinford6372</td>\n",
       "      <td>stage</td>\n",
       "      <td>stag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96593</th>\n",
       "      <td>eAnVFKndFoY</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>greta thunberg   waste  skin  oxygen</td>\n",
       "      <td>@stevenewberry6460</td>\n",
       "      <td>greta thunberg wast skin oxygen</td>\n",
       "      <td>greta thunberg waste skin oxygen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96594</th>\n",
       "      <td>eAnVFKndFoY</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>104</td>\n",
       "      <td>put  hard hat   ostrich   make   construction worker</td>\n",
       "      <td>@sgj8929</td>\n",
       "      <td>put hard hat ostrich make construct worker</td>\n",
       "      <td>put hard hat ostrich make construction worker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96595 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id published_at  like_count  \\\n",
       "0      uW6fi2tCnAc   2023-02-19           1   \n",
       "1      uW6fi2tCnAc   2023-02-19           2   \n",
       "2      uW6fi2tCnAc   2023-02-18           4   \n",
       "3      uW6fi2tCnAc   2023-02-18           0   \n",
       "4      uW6fi2tCnAc   2023-02-18           3   \n",
       "...            ...          ...         ...   \n",
       "96590  eAnVFKndFoY   2023-01-23           0   \n",
       "96591  eAnVFKndFoY   2023-01-23           4   \n",
       "96592  eAnVFKndFoY   2023-01-23           5   \n",
       "96593  eAnVFKndFoY   2023-01-23           1   \n",
       "96594  eAnVFKndFoY   2023-01-23         104   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0                                                                                                                                                                                                                                                              answer   china  india  help   matter  much money  rest   world throws  reducing carbon footprint  complete waste  trillion dollars ü§¶‚Äç‚ôÄÔ∏è   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                guy   expert  screwed   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                        kennedy   gem   \n",
       "3           get  nation like china  cooperate  even trust    telling  truth   say    absolute ridiculousness   occurring  western nations supposedly   advanced  capable   leads  science  technology  willing  cripple  impoverish millions   sake  iffy pseudo science     experts care anyway  wealthy elitists    least affected   ramifications   extreme measures   would promote   rest   world   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                   man  going   oscar   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                ...   \n",
       "96590                                                                                                                                                                                                                                                                                                                    course ‚Äô theatre global warming   hoax  use  fear  control low iq individuals   \n",
       "96591                                                                                                                                                                                                                                                                                                                                                                                              yup   \n",
       "96592                                                                                                                                                                                                                                                                                                                                                                                           staged   \n",
       "96593                                                                                                                                                                                                                                                                                                                                                             greta thunberg   waste  skin  oxygen   \n",
       "96594                                                                                                                                                                                                                                                                                                                                             put  hard hat   ostrich   make   construction worker   \n",
       "\n",
       "                      author  \\\n",
       "0                  @ilandgrl   \n",
       "1             @lobusdiMortis   \n",
       "2                 @skitz1337   \n",
       "3              @dodieodie498   \n",
       "4                @karenh4458   \n",
       "...                      ...   \n",
       "96590  @internetexplorer6097   \n",
       "96591       @bonitabeach3127   \n",
       "96592         @kevinford6372   \n",
       "96593     @stevenewberry6460   \n",
       "96594               @sgj8929   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    stemmed_text  \\\n",
       "0                                                                                                                                                                            answer china india help matter much money rest world throw reduc carbon footprint complet wast trillion dollar ü§¶‚Äç‚ôÄÔ∏è   \n",
       "1                                                                                                                                                                                                                                                                               guy expert screw   \n",
       "2                                                                                                                                                                                                                                                                                    kennedi gem   \n",
       "3      get nation like china cooper even trust tell truth say absolut ridicul occur western nation suppos advanc capabl lead scienc technolog will crippl impoverish million sake iffi pseudo scienc expert care anyway wealthi elitist least affect ramif extrem measur would promot rest world   \n",
       "4                                                                                                                                                                                                                                                                                   man go oscar   \n",
       "...                                                                                                                                                                                                                                                                                          ...   \n",
       "96590                                                                                                                                                                                                                           cours ‚Äô theatr global warm hoax use fear control low iq individu   \n",
       "96591                                                                                                                                                                                                                                                                                        yup   \n",
       "96592                                                                                                                                                                                                                                                                                      stage   \n",
       "96593                                                                                                                                                                                                                                                            greta thunberg wast skin oxygen   \n",
       "96594                                                                                                                                                                                                                                                 put hard hat ostrich make construct worker   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                     lemmatized_text  \n",
       "0                                                                                                                                                                                                             answer china india help matter much money rest world throw reduce carbon footprint complete waste trillion dollar ü§¶‚Äç‚ôÄÔ∏è  \n",
       "1                                                                                                                                                                                                                                                                                                                   guy expert screw  \n",
       "2                                                                                                                                                                                                                                                                                                                        kennedy gem  \n",
       "3      get nation like china cooperate even trust tell truth say absolute ridiculousness occur western nation supposedly advance capable lead science technology willing cripple impoverish million sake iffy pseudo science expert care anyway wealthy elitist least affected ramification extreme measure would promote rest world  \n",
       "4                                                                                                                                                                                                                                                                                                                       man go oscar  \n",
       "...                                                                                                                                                                                                                                                                                                                              ...  \n",
       "96590                                                                                                                                                                                                                                                           course ‚Äô theatre global warm hoax use fear control low iq individual  \n",
       "96591                                                                                                                                                                                                                                                                                                                            yup  \n",
       "96592                                                                                                                                                                                                                                                                                                                           stag  \n",
       "96593                                                                                                                                                                                                                                                                                               greta thunberg waste skin oxygen  \n",
       "96594                                                                                                                                                                                                                                                                                  put hard hat ostrich make construction worker  \n",
       "\n",
       "[96595 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_date_format(df, column_name):\n",
    "    # Convert the column to datetime\n",
    "    df[column_name] = pd.to_datetime(df[column_name])\n",
    "    \n",
    "    # Format the dates back to 'dd mm yyyy' strings\n",
    "    df[column_name] = df[column_name].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return df\n",
    "\n",
    "processed_df = convert_date_format(processed_df, 'published_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7333, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7333/7333 [00:05<00:00, 1332.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. Replacing NaN-values and aggregating data by date\n",
    "processed_df.lemmatized_text = processed_df.lemmatized_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "processed_df.stemmed_text = processed_df.stemmed_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "\n",
    "\n",
    "#Aggregating tweet data by dates and affiliation \n",
    "# agg() is an aggregation function in shape of a dictionary in which we specify which variables (documents here) we want aggregated (joined)\n",
    "df_agg = processed_df.groupby(['published_at', 'video_id'], as_index = False).agg({'text': ' '.join, #as_index= False, so these variables are not the index\n",
    "                                                                            'lemmatized_text': ' '.join, # documents are joined with a spacce\n",
    "                                                                            'stemmed_text': ' '.join})\n",
    "#Checking dimensions of new dataset and viewing the dataset\n",
    "print(df_agg.shape)\n",
    "\n",
    "# 4. Tokenizing stemmed text\n",
    "\n",
    "#Defining NLTK's TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tqdm.pandas() #Creates a progress bar and below use \"progress_apply\" instead of \"apply\" to create a progress bar (This is more of a \"nice to have\" than a \"need to have\")\n",
    "\n",
    "#Tokenizing and creating a column of unigrams from the stemmed tweet text. \n",
    "df_agg['unigrams'] = df_agg['stemmed_text'].progress_apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7333/7333 [00:00<00:00, 27893.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#Defining a function that will create bigrams \n",
    "def bigrams(doc): # a doc is a list of unigrams in same order as in tweets \n",
    "    \n",
    "    bigrams = [] #Empty list to save the bigrams\n",
    "    \n",
    "    for bigram in list(nltk.bigrams(doc)):  #Creating bigrams as tuples with nltk.bigrams and iterating over these them\n",
    "        bigrams.append(\"_\".join(bigram))    #Joining each bigram-tuple pair with an underscore and saving to list\n",
    "    \n",
    "    return bigrams\n",
    "\n",
    "#Creating a column with bigrams by applying function to column of unigrams\n",
    "df_agg['bigrams'] = df_agg.unigrams.progress_apply(lambda x: bigrams(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg['tokens'] = df_agg.unigrams+df_agg.bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "using topic modelling to explore for keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896734\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a id2word dictionary\n",
    "\n",
    "#Insert the column where you saved unigram and bigram tokens between the parentheses\n",
    "id2word = Dictionary(df_agg['tokens']) # A dictionary is created through which each token gets a unique id \n",
    "\n",
    "#Viewing how many words are in our vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19198\n"
     ]
    }
   ],
   "source": [
    "# 2. Removing very frequent and infrequent words\n",
    "id2word.filter_extremes(no_below=10, #filtering out words appearing in less than 10 documents\n",
    "                        no_above=.999, #filtering out words appearing in more than 99,9% of all documents\n",
    "                        keep_n=None) # If we don't set keep_n=None, then the vocabulary \"only\" contains the 10000 most frequent words \n",
    "\n",
    "#Viewing how many words are now in our vocabulary after filtering\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Creating corpus\n",
    "\n",
    "# Convert documents into the bag-of-words (BoW) format. \n",
    "# We get a list of tuples with (token_id, token_count) for each document.\n",
    "corpus = [id2word.doc2bow(doc) for doc in df_agg['tokens']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = LsiModel(corpus=corpus, num_topics=10, id2word=id2word)\n",
    "topics = lsi_model.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.387*\"peopl\" + 0.306*\"‚Äô\" + 0.229*\"govern\" + 0.184*\"land\" + 0.165*\"fire\" + 0.162*\"like\" + 0.155*\"get\" + 0.128*\"go\" + 0.122*\"climat\" + 0.117*\"need\"'),\n",
       " (1,\n",
       "  '0.507*\"climat\" + 0.341*\"chang\" + 0.264*\"climat_chang\" + -0.187*\"land\" + -0.174*\"govern\" + -0.157*\"fire\" + -0.143*\"peopl\" + 0.132*\"year\" + -0.126*\"maui\" + -0.124*\"tulsi\"'),\n",
       " (2,\n",
       "  '-0.289*\"kerri\" + 0.260*\"chang\" + -0.222*\"üòÇ\" + 0.210*\"climat\" + -0.209*\"fuel\" + 0.197*\"climat_chang\" + -0.176*\"fossil\" + -0.169*\"lie\" + -0.159*\"fossil_fuel\" + -0.152*\"like\"'),\n",
       " (3,\n",
       "  '0.427*\"kerri\" + -0.304*\"fuel\" + -0.279*\"fossil\" + -0.252*\"fossil_fuel\" + 0.211*\"john\" + 0.172*\"lie\" + 0.171*\"john_kerri\" + 0.159*\"fli\" + -0.152*\"oil\" + -0.136*\"energi\"'),\n",
       " (4,\n",
       "  '0.340*\"üòÇ\" + 0.266*\"expert\" + 0.203*\"kid\" + 0.190*\"chang\" + -0.169*\"kerri\" + -0.159*\"thank\" + 0.156*\"üòÇ_üòÇ\" + -0.153*\"toni\" + 0.142*\"greta\" + 0.140*\"climat_chang\"'),\n",
       " (5,\n",
       "  '-0.263*\"greta\" + -0.202*\"expert\" + -0.185*\"know\" + -0.181*\"co\" + 0.171*\"climat_chang\" + 0.170*\"chang\" + 0.165*\"climat\" + -0.162*\"toni\" + 0.140*\"fuel\" + -0.128*\"thank\"'),\n",
       " (6,\n",
       "  '0.347*\"cloud\" + 0.319*\"seed\" + 0.258*\"cloud_seed\" + 0.211*\"weather\" + 0.150*\"rain\" + -0.135*\"expert\" + -0.116*\"money\" + -0.114*\"chang\" + 0.112*\"sun\" + 0.111*\"year\"'),\n",
       " (7,\n",
       "  '-0.518*\"greta\" + 0.218*\"co\" + 0.166*\"expert\" + 0.144*\"toni\" + 0.130*\"year\" + -0.123*\"russia\" + -0.119*\"climat_chang\" + 0.119*\"temperatur\" + 0.116*\"carbon\" + -0.113*\"nuclear\"'),\n",
       " (8,\n",
       "  '0.361*\"greta\" + 0.283*\"toni\" + -0.280*\"cloud\" + -0.273*\"seed\" + -0.220*\"cloud_seed\" + -0.166*\"‚Äô\" + 0.126*\"lie\" + 0.116*\"thank\" + -0.111*\"peterson\" + -0.109*\"dr\"'),\n",
       " (9,\n",
       "  '-0.479*\"toni\" + 0.171*\"greta\" + -0.161*\"thank\" + 0.157*\"co\" + 0.149*\"earth\" + -0.147*\"climat\" + -0.140*\"thank_toni\" + 0.130*\"sun\" + 0.128*\"wef\" + -0.126*\"seed\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*climat + 0.008*peopl + 0.008*‚Äô + 0.007*chang + 0.005*like + 0.005*climat_chang + 0.005*year + 0.004*get + 0.004*one + 0.004*would + 0.004*go + 0.004*know + 0.003*us + 0.003*need + 0.003*co + 0.003*say + 0.003*make + 0.003*time + 0.003*use + 0.003*thank'),\n",
       " (1,\n",
       "  '0.010*climat + 0.006*chang + 0.004*like + 0.004*peopl + 0.004*year + 0.004*climat_chang + 0.004*‚Äô + 0.004*co + 0.004*would + 0.003*one + 0.003*scienc + 0.003*oil + 0.003*warm + 0.003*earth + 0.003*energi + 0.003*thank + 0.003*get + 0.003*time + 0.003*say + 0.003*fossil'),\n",
       " (2,\n",
       "  '0.008*climat + 0.005*‚Äô + 0.005*chang + 0.004*peopl + 0.004*like + 0.004*energi + 0.004*climat_chang + 0.003*one + 0.003*would + 0.003*year + 0.003*get + 0.003*need + 0.003*thank + 0.003*world + 0.003*use + 0.003*know + 0.003*go + 0.002*time + 0.002*us + 0.002*make'),\n",
       " (3,\n",
       "  '0.009*climat + 0.006*chang + 0.005*‚Äô + 0.005*climat_chang + 0.004*peopl + 0.004*thank + 0.003*year + 0.003*would + 0.003*one + 0.003*like + 0.003*co + 0.003*scienc + 0.003*think + 0.003*time + 0.002*get + 0.002*know + 0.002*earth + 0.002*much + 0.002*go + 0.002*say'),\n",
       " (4,\n",
       "  '0.007*climat + 0.005*chang + 0.004*‚Äô + 0.004*peopl + 0.004*like + 0.004*would + 0.003*year + 0.003*climat_chang + 0.003*co + 0.003*one + 0.003*get + 0.002*make + 0.002*scientist + 0.002*need + 0.002*time + 0.002*energi + 0.002*know + 0.002*power + 0.002*earth + 0.002*think'),\n",
       " (5,\n",
       "  '0.008*climat + 0.006*‚Äô + 0.005*chang + 0.005*year + 0.004*climat_chang + 0.003*like + 0.003*peopl + 0.003*get + 0.003*go + 0.003*world + 0.002*one + 0.002*would + 0.002*toni + 0.002*thank + 0.002*lie + 0.002*us + 0.002*global + 0.002*time + 0.002*need + 0.002*make'),\n",
       " (6,\n",
       "  '0.007*climat + 0.005*chang + 0.004*‚Äô + 0.004*climat_chang + 0.004*peopl + 0.004*like + 0.003*one + 0.003*year + 0.003*know + 0.003*go + 0.003*say + 0.003*get + 0.002*would + 0.002*need + 0.002*time + 0.002*world + 0.002*think + 0.002*co + 0.002*use + 0.002*thing'),\n",
       " (7,\n",
       "  '0.007*climat + 0.004*chang + 0.003*peopl + 0.003*‚Äô + 0.003*climat_chang + 0.003*dr + 0.002*co + 0.002*thank + 0.002*like + 0.002*one + 0.002*scienc + 0.002*would + 0.002*global + 0.002*use + 0.002*lindzen + 0.002*scientist + 0.002*make + 0.002*say + 0.002*year + 0.002*temperatur'),\n",
       " (8,\n",
       "  '0.006*climat + 0.004*chang + 0.003*year + 0.003*peopl + 0.003*co + 0.003*climat_chang + 0.003*thank + 0.003*like + 0.002*one + 0.002*toni + 0.002*go + 0.002*‚Äô + 0.002*scienc + 0.002*would + 0.002*video + 0.002*human + 0.002*say + 0.002*think + 0.002*great + 0.002*global'),\n",
       " (9,\n",
       "  '0.006*climat + 0.003*‚Äô + 0.003*chang + 0.003*peopl + 0.003*energi + 0.003*like + 0.002*one + 0.002*would + 0.002*climat_chang + 0.002*world + 0.002*dr + 0.002*thank + 0.002*year + 0.002*know + 0.002*peterson + 0.002*need + 0.002*us + 0.002*get + 0.002*use + 0.002*jordan')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp_model = HdpModel(corpus=corpus, id2word=id2word)\n",
    "hdp_model.show_topics()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"‚Äô\" + 0.008*\"peopl\" + 0.007*\"climat\" + 0.006*\"get\" + 0.005*\"would\" + 0.005*\"like\" + 0.005*\"go\" + 0.005*\"us\" + 0.005*\"know\" + 0.004*\"fire\"'),\n",
       " (1,\n",
       "  '0.015*\"climat\" + 0.011*\"chang\" + 0.009*\"peopl\" + 0.009*\"‚Äô\" + 0.008*\"climat_chang\" + 0.007*\"year\" + 0.006*\"like\" + 0.005*\"go\" + 0.004*\"one\" + 0.004*\"get\"'),\n",
       " (2,\n",
       "  '0.018*\"climat\" + 0.009*\"chang\" + 0.008*\"toni\" + 0.008*\"thank\" + 0.006*\"year\" + 0.006*\"climat_chang\" + 0.005*\"‚Äô\" + 0.005*\"peopl\" + 0.004*\"one\" + 0.004*\"co\"'),\n",
       " (3,\n",
       "  '0.014*\"‚Äô\" + 0.013*\"peopl\" + 0.009*\"govern\" + 0.008*\"get\" + 0.007*\"like\" + 0.006*\"one\" + 0.005*\"say\" + 0.005*\"know\" + 0.005*\"climat\" + 0.004*\"need\"'),\n",
       " (4,\n",
       "  '0.009*\"‚Äô\" + 0.008*\"peopl\" + 0.008*\"climat\" + 0.006*\"like\" + 0.006*\"know\" + 0.006*\"chang\" + 0.005*\"want\" + 0.004*\"year\" + 0.004*\"climat_chang\" + 0.004*\"say\"'),\n",
       " (5,\n",
       "  '0.010*\"üòÇ\" + 0.008*\"climat\" + 0.005*\"üòÇ_üòÇ\" + 0.005*\"thank\" + 0.005*\"chang\" + 0.005*\"‚Äô\" + 0.004*\"peopl\" + 0.004*\"say\" + 0.004*\"climat_chang\" + 0.004*\"ü§£\"'),\n",
       " (6,\n",
       "  '0.010*\"climat\" + 0.009*\"peopl\" + 0.005*\"‚Äô\" + 0.005*\"chang\" + 0.005*\"get\" + 0.004*\"time\" + 0.004*\"year\" + 0.004*\"global\" + 0.004*\"need\" + 0.004*\"toni\"'),\n",
       " (7,\n",
       "  '0.012*\"climat\" + 0.010*\"chang\" + 0.008*\"climat_chang\" + 0.005*\"year\" + 0.005*\"earth\" + 0.005*\"co\" + 0.004*\"scienc\" + 0.004*\"human\" + 0.004*\"like\" + 0.004*\"fuel\"'),\n",
       " (8,\n",
       "  '0.012*\"co\" + 0.011*\"climat\" + 0.006*\"temperatur\" + 0.005*\"‚Äô\" + 0.005*\"peopl\" + 0.005*\"energi\" + 0.005*\"would\" + 0.004*\"carbon\" + 0.004*\"use\" + 0.004*\"earth\"'),\n",
       " (9,\n",
       "  '0.010*\"climat\" + 0.009*\"‚Äô\" + 0.008*\"peopl\" + 0.008*\"chang\" + 0.006*\"fossil\" + 0.006*\"fossil_fuel\" + 0.006*\"fuel\" + 0.006*\"like\" + 0.005*\"climat_chang\" + 0.005*\"know\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(corpus=corpus, num_topics=10, id2word=id2word)\n",
    "lda_model.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Methods - Dictionary Classifier\n",
    "_____\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Libraries](#libraries)\n",
    "2. [Load Data](#load-data)\n",
    "3. [Data Preprocessing](#preprocessing-of-the-data)\n",
    "4. [Set Up of Dictionary](#building-dictionary)\n",
    "5. [Classifier](#classifier)\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "All libraries which are needed to execute the code are listed here. Install the packages by using the `requirements.txt` file. \n",
    "\n",
    "The documentation can be found in the [README.md](README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load collected YouTube comments from the Data Collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/comments_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data with using functions from functions.py\n",
    "processed_df = (\n",
    "    df.pipe(remove_users, 'text')\n",
    "      .pipe(lowercase_text, 'text')\n",
    "      .pipe(remove_whitespace, 'text')\n",
    "      .pipe(remove_punctuation, 'text')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19T21:22:45Z</td>\n",
       "      <td>1</td>\n",
       "      <td>the answer is if china and india dont help it ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19T00:43:40Z</td>\n",
       "      <td>2</td>\n",
       "      <td>and that guy is an expert were screwed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18T22:57:38Z</td>\n",
       "      <td>4</td>\n",
       "      <td>kennedy is a gem</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18T22:22:49Z</td>\n",
       "      <td>0</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18T21:44:49Z</td>\n",
       "      <td>3</td>\n",
       "      <td>that man was going for an oscar</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id          published_at  like_count  \\\n",
       "0  uW6fi2tCnAc  2023-02-19T21:22:45Z           1   \n",
       "1  uW6fi2tCnAc  2023-02-19T00:43:40Z           2   \n",
       "2  uW6fi2tCnAc  2023-02-18T22:57:38Z           4   \n",
       "3  uW6fi2tCnAc  2023-02-18T22:22:49Z           0   \n",
       "4  uW6fi2tCnAc  2023-02-18T21:44:49Z           3   \n",
       "\n",
       "                                                text  author  \n",
       "0  the answer is if china and india dont help it ...     0.0  \n",
       "1             and that guy is an expert were screwed     1.0  \n",
       "2                                   kennedy is a gem     2.0  \n",
       "3  and just how do we get a nation like china to ...     3.0  \n",
       "4                    that man was going for an oscar     4.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# heading the processed df (removed users, lowercased, removed whitspace and punctuation, stemmed and lemmatized)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the Data\n",
    "\n",
    "- Stemming and Lemmatizing\n",
    "- Tokenization of the comments\n",
    "- Building N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text column to string\n",
    "processed_df['text'] = processed_df['text'].astype('str')\n",
    "processed_df['text'] = processed_df['text'].str.replace('\\'', '')\n",
    "\n",
    "# use stemming to reduce words to their root words\n",
    "processed_df = stem_words(processed_df, 'text')\n",
    "\n",
    "# use lemmatization to reduce words to their root form\n",
    "processed_df = lemmatize_words(processed_df, 'text')\n",
    "\n",
    "# convert date format\n",
    "processed_df = convert_date_format(processed_df, 'published_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the na from lemmatized & stemmed text to avoid issues with creating n_grams\n",
    "processed_df.lemmatized_text = processed_df.lemmatized_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "processed_df.stemmed_text = processed_df.stemmed_text.apply(lambda x: '' if str(x) == 'nan' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>1</td>\n",
       "      <td>the answer is if china and india dont help it ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the answer is if china and india dont help it ...</td>\n",
       "      <td>the answer be if china and india dont help it ...</td>\n",
       "      <td>[the, answer, is, if, china, and, india, dont,...</td>\n",
       "      <td>[the, answer, be, if, china, and, india, dont,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>2</td>\n",
       "      <td>and that guy is an expert were screwed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>and that guy is an expert were screw</td>\n",
       "      <td>and that guy be an expert be screw</td>\n",
       "      <td>[and, that, guy, is, an, expert, were, screw]</td>\n",
       "      <td>[and, that, guy, be, an, expert, be, screw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>4</td>\n",
       "      <td>kennedy is a gem</td>\n",
       "      <td>2.0</td>\n",
       "      <td>kennedi is a gem</td>\n",
       "      <td>kennedy be a gem</td>\n",
       "      <td>[kennedi, is, a, gem]</td>\n",
       "      <td>[kennedy, be, a, gem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>0</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>that man was going for an oscar</td>\n",
       "      <td>4.0</td>\n",
       "      <td>that man was go for an oscar</td>\n",
       "      <td>that man be go for an oscar</td>\n",
       "      <td>[that, man, was, go, for, an, oscar]</td>\n",
       "      <td>[that, man, be, go, for, an, oscar]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id published_at  like_count  \\\n",
       "0  uW6fi2tCnAc   2023-02-19           1   \n",
       "1  uW6fi2tCnAc   2023-02-19           2   \n",
       "2  uW6fi2tCnAc   2023-02-18           4   \n",
       "3  uW6fi2tCnAc   2023-02-18           0   \n",
       "4  uW6fi2tCnAc   2023-02-18           3   \n",
       "\n",
       "                                                text  author  \\\n",
       "0  the answer is if china and india dont help it ...     0.0   \n",
       "1             and that guy is an expert were screwed     1.0   \n",
       "2                                   kennedy is a gem     2.0   \n",
       "3  and just how do we get a nation like china to ...     3.0   \n",
       "4                    that man was going for an oscar     4.0   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  the answer is if china and india dont help it ...   \n",
       "1               and that guy is an expert were screw   \n",
       "2                                   kennedi is a gem   \n",
       "3  and just how do we get a nation like china to ...   \n",
       "4                       that man was go for an oscar   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  the answer be if china and india dont help it ...   \n",
       "1                 and that guy be an expert be screw   \n",
       "2                                   kennedy be a gem   \n",
       "3  and just how do we get a nation like china to ...   \n",
       "4                        that man be go for an oscar   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [the, answer, is, if, china, and, india, dont,...   \n",
       "1      [and, that, guy, is, an, expert, were, screw]   \n",
       "2                              [kennedi, is, a, gem]   \n",
       "3  [and, just, how, do, we, get, a, nation, like,...   \n",
       "4               [that, man, was, go, for, an, oscar]   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0  [the, answer, be, if, china, and, india, dont,...  \n",
       "1        [and, that, guy, be, an, expert, be, screw]  \n",
       "2                              [kennedy, be, a, gem]  \n",
       "3  [and, just, how, do, we, get, a, nation, like,...  \n",
       "4                [that, man, be, go, for, an, oscar]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing the lemmatized and stemmed text before creating n-grams\n",
    "\n",
    "def tokenize_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    #words_with_quotes = [f\"'{word}'\" for word in words]\n",
    "    return words\n",
    "\n",
    "processed_df[\"stemmed_tokens\"] = processed_df[\"stemmed_text\"].apply(lambda x: tokenize_words(x))\n",
    "processed_df[\"lemmatized_tokens\"] = processed_df[\"lemmatized_text\"].apply(lambda x: tokenize_words(x))\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96595/96595 [00:01<00:00, 69386.92it/s] \n",
      "100%|██████████| 96595/96595 [00:00<00:00, 106254.24it/s]\n",
      "100%|██████████| 96595/96595 [00:01<00:00, 85811.63it/s] \n",
      "100%|██████████| 96595/96595 [00:00<00:00, 150662.73it/s]\n",
      "100%|██████████| 96595/96595 [00:00<00:00, 115284.99it/s]\n",
      "100%|██████████| 96595/96595 [00:01<00:00, 78540.57it/s] \n"
     ]
    }
   ],
   "source": [
    "# Creating n-grams \n",
    "tqdm.pandas() #Creates a progress bar and below use \"progress_apply\" instead of \"apply\" to create a progress bar (This is more of a \"nice to have\" than a \"need to have\")\n",
    "\n",
    "#Defining a function that will create bigrams \n",
    "def bigrams(doc): # a doc is a list of tokens/unigrams in same order as in tweets \n",
    "    \n",
    "    bigrams = [] #Empty list to save the bigrams\n",
    "    \n",
    "    for bigram in list(nltk.bigrams(doc)):  #Creating bigrams as tuples with nltk.bigrams and iterating over these them\n",
    "        bigrams.append(\"_\".join(bigram))    #Joining each bigram-tuple pair with an underscore and saving to list\n",
    "    \n",
    "    return bigrams\n",
    "\n",
    "#Defining a function that will create bigrams \n",
    "def trigrams(doc): # a doc is a list of unigrams in same order as in tweets \n",
    "    \n",
    "    trigrams = [] #Empty list to save the bigrams\n",
    "    \n",
    "    for trigram in list(nltk.trigrams(doc)):  #Creating bigrams as tuples with nltk.bigrams and iterating over these them\n",
    "        trigrams.append(\"_\".join(trigram))    #Joining each bigram-tuple pair with an underscore and saving to list\n",
    "    \n",
    "    return trigrams\n",
    "\n",
    "#Defining a function that will create bigrams \n",
    "def fourgrams(doc): # a doc is a list of unigrams in same order as in tweets \n",
    "    \n",
    "    fourgrams = [] #Empty list to save the bigrams\n",
    "    \n",
    "    for fourgram in list(ngrams(doc, 4)):  #Creating bigrams as tuples with nltk.bigrams and iterating over these them\n",
    "        fourgrams.append(\"_\".join(fourgram))    #Joining each bigram-tuple pair with an underscore and saving to list\n",
    "    \n",
    "    return fourgrams\n",
    "\n",
    "#Creating a column with bigrams by applying function to column of unigrams\n",
    "processed_df['bigrams_lemma'] = processed_df[\"lemmatized_tokens\"].progress_apply(lambda x: bigrams(x))\n",
    "processed_df['trigrams_lemma'] = processed_df['lemmatized_tokens'].progress_apply(lambda x : trigrams(x))\n",
    "processed_df['fourgrams_lemma'] = processed_df['lemmatized_tokens'].progress_apply(lambda x : fourgrams(x))\n",
    "\n",
    "processed_df['bigrams_stem'] = processed_df[\"stemmed_tokens\"].progress_apply(lambda x: bigrams(x))\n",
    "processed_df['trigrams_stem'] = processed_df['stemmed_tokens'].progress_apply(lambda x : trigrams(x))\n",
    "processed_df['fourgrams_stem'] = processed_df['stemmed_tokens'].progress_apply(lambda x : fourgrams(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>bigrams_lemma</th>\n",
       "      <th>trigrams_lemma</th>\n",
       "      <th>fourgrams_lemma</th>\n",
       "      <th>bigrams_stem</th>\n",
       "      <th>trigrams_stem</th>\n",
       "      <th>fourgrams_stem</th>\n",
       "      <th>all_n_grams_lemmatized</th>\n",
       "      <th>all_n_grams_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>1</td>\n",
       "      <td>the answer is if china and india dont help it ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the answer is if china and india dont help it ...</td>\n",
       "      <td>the answer be if china and india dont help it ...</td>\n",
       "      <td>[the, answer, is, if, china, and, india, dont,...</td>\n",
       "      <td>[the, answer, be, if, china, and, india, dont,...</td>\n",
       "      <td>[the_answer, answer_be, be_if, if_china, china...</td>\n",
       "      <td>[the_answer_be, answer_be_if, be_if_china, if_...</td>\n",
       "      <td>[the_answer_be_if, answer_be_if_china, be_if_c...</td>\n",
       "      <td>[the_answer, answer_is, is_if, if_china, china...</td>\n",
       "      <td>[the_answer_is, answer_is_if, is_if_china, if_...</td>\n",
       "      <td>[the_answer_is_if, answer_is_if_china, is_if_c...</td>\n",
       "      <td>[the, answer, be, if, china, and, india, dont,...</td>\n",
       "      <td>[the, answer, is, if, china, and, india, dont,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>2</td>\n",
       "      <td>and that guy is an expert were screwed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>and that guy is an expert were screw</td>\n",
       "      <td>and that guy be an expert be screw</td>\n",
       "      <td>[and, that, guy, is, an, expert, were, screw]</td>\n",
       "      <td>[and, that, guy, be, an, expert, be, screw]</td>\n",
       "      <td>[and_that, that_guy, guy_be, be_an, an_expert,...</td>\n",
       "      <td>[and_that_guy, that_guy_be, guy_be_an, be_an_e...</td>\n",
       "      <td>[and_that_guy_be, that_guy_be_an, guy_be_an_ex...</td>\n",
       "      <td>[and_that, that_guy, guy_is, is_an, an_expert,...</td>\n",
       "      <td>[and_that_guy, that_guy_is, guy_is_an, is_an_e...</td>\n",
       "      <td>[and_that_guy_is, that_guy_is_an, guy_is_an_ex...</td>\n",
       "      <td>[and, that, guy, be, an, expert, be, screw, an...</td>\n",
       "      <td>[and, that, guy, is, an, expert, were, screw, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>4</td>\n",
       "      <td>kennedy is a gem</td>\n",
       "      <td>2.0</td>\n",
       "      <td>kennedi is a gem</td>\n",
       "      <td>kennedy be a gem</td>\n",
       "      <td>[kennedi, is, a, gem]</td>\n",
       "      <td>[kennedy, be, a, gem]</td>\n",
       "      <td>[kennedy_be, be_a, a_gem]</td>\n",
       "      <td>[kennedy_be_a, be_a_gem]</td>\n",
       "      <td>[kennedy_be_a_gem]</td>\n",
       "      <td>[kennedi_is, is_a, a_gem]</td>\n",
       "      <td>[kennedi_is_a, is_a_gem]</td>\n",
       "      <td>[kennedi_is_a_gem]</td>\n",
       "      <td>[kennedy, be, a, gem, kennedy_be, be_a, a_gem,...</td>\n",
       "      <td>[kennedi, is, a, gem, kennedi_is, is_a, a_gem,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>0</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "      <td>[and_just, just_how, how_do, do_we, we_get, ge...</td>\n",
       "      <td>[and_just_how, just_how_do, how_do_we, do_we_g...</td>\n",
       "      <td>[and_just_how_do, just_how_do_we, how_do_we_ge...</td>\n",
       "      <td>[and_just, just_how, how_do, do_we, we_get, ge...</td>\n",
       "      <td>[and_just_how, just_how_do, how_do_we, do_we_g...</td>\n",
       "      <td>[and_just_how_do, just_how_do_we, how_do_we_ge...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>that man was going for an oscar</td>\n",
       "      <td>4.0</td>\n",
       "      <td>that man was go for an oscar</td>\n",
       "      <td>that man be go for an oscar</td>\n",
       "      <td>[that, man, was, go, for, an, oscar]</td>\n",
       "      <td>[that, man, be, go, for, an, oscar]</td>\n",
       "      <td>[that_man, man_be, be_go, go_for, for_an, an_o...</td>\n",
       "      <td>[that_man_be, man_be_go, be_go_for, go_for_an,...</td>\n",
       "      <td>[that_man_be_go, man_be_go_for, be_go_for_an, ...</td>\n",
       "      <td>[that_man, man_was, was_go, go_for, for_an, an...</td>\n",
       "      <td>[that_man_was, man_was_go, was_go_for, go_for_...</td>\n",
       "      <td>[that_man_was_go, man_was_go_for, was_go_for_a...</td>\n",
       "      <td>[that, man, be, go, for, an, oscar, that_man, ...</td>\n",
       "      <td>[that, man, was, go, for, an, oscar, that_man,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id published_at  like_count  \\\n",
       "0  uW6fi2tCnAc   2023-02-19           1   \n",
       "1  uW6fi2tCnAc   2023-02-19           2   \n",
       "2  uW6fi2tCnAc   2023-02-18           4   \n",
       "3  uW6fi2tCnAc   2023-02-18           0   \n",
       "4  uW6fi2tCnAc   2023-02-18           3   \n",
       "\n",
       "                                                text  author  \\\n",
       "0  the answer is if china and india dont help it ...     0.0   \n",
       "1             and that guy is an expert were screwed     1.0   \n",
       "2                                   kennedy is a gem     2.0   \n",
       "3  and just how do we get a nation like china to ...     3.0   \n",
       "4                    that man was going for an oscar     4.0   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  the answer is if china and india dont help it ...   \n",
       "1               and that guy is an expert were screw   \n",
       "2                                   kennedi is a gem   \n",
       "3  and just how do we get a nation like china to ...   \n",
       "4                       that man was go for an oscar   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  the answer be if china and india dont help it ...   \n",
       "1                 and that guy be an expert be screw   \n",
       "2                                   kennedy be a gem   \n",
       "3  and just how do we get a nation like china to ...   \n",
       "4                        that man be go for an oscar   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [the, answer, is, if, china, and, india, dont,...   \n",
       "1      [and, that, guy, is, an, expert, were, screw]   \n",
       "2                              [kennedi, is, a, gem]   \n",
       "3  [and, just, how, do, we, get, a, nation, like,...   \n",
       "4               [that, man, was, go, for, an, oscar]   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [the, answer, be, if, china, and, india, dont,...   \n",
       "1        [and, that, guy, be, an, expert, be, screw]   \n",
       "2                              [kennedy, be, a, gem]   \n",
       "3  [and, just, how, do, we, get, a, nation, like,...   \n",
       "4                [that, man, be, go, for, an, oscar]   \n",
       "\n",
       "                                       bigrams_lemma  \\\n",
       "0  [the_answer, answer_be, be_if, if_china, china...   \n",
       "1  [and_that, that_guy, guy_be, be_an, an_expert,...   \n",
       "2                          [kennedy_be, be_a, a_gem]   \n",
       "3  [and_just, just_how, how_do, do_we, we_get, ge...   \n",
       "4  [that_man, man_be, be_go, go_for, for_an, an_o...   \n",
       "\n",
       "                                      trigrams_lemma  \\\n",
       "0  [the_answer_be, answer_be_if, be_if_china, if_...   \n",
       "1  [and_that_guy, that_guy_be, guy_be_an, be_an_e...   \n",
       "2                           [kennedy_be_a, be_a_gem]   \n",
       "3  [and_just_how, just_how_do, how_do_we, do_we_g...   \n",
       "4  [that_man_be, man_be_go, be_go_for, go_for_an,...   \n",
       "\n",
       "                                     fourgrams_lemma  \\\n",
       "0  [the_answer_be_if, answer_be_if_china, be_if_c...   \n",
       "1  [and_that_guy_be, that_guy_be_an, guy_be_an_ex...   \n",
       "2                                 [kennedy_be_a_gem]   \n",
       "3  [and_just_how_do, just_how_do_we, how_do_we_ge...   \n",
       "4  [that_man_be_go, man_be_go_for, be_go_for_an, ...   \n",
       "\n",
       "                                        bigrams_stem  \\\n",
       "0  [the_answer, answer_is, is_if, if_china, china...   \n",
       "1  [and_that, that_guy, guy_is, is_an, an_expert,...   \n",
       "2                          [kennedi_is, is_a, a_gem]   \n",
       "3  [and_just, just_how, how_do, do_we, we_get, ge...   \n",
       "4  [that_man, man_was, was_go, go_for, for_an, an...   \n",
       "\n",
       "                                       trigrams_stem  \\\n",
       "0  [the_answer_is, answer_is_if, is_if_china, if_...   \n",
       "1  [and_that_guy, that_guy_is, guy_is_an, is_an_e...   \n",
       "2                           [kennedi_is_a, is_a_gem]   \n",
       "3  [and_just_how, just_how_do, how_do_we, do_we_g...   \n",
       "4  [that_man_was, man_was_go, was_go_for, go_for_...   \n",
       "\n",
       "                                      fourgrams_stem  \\\n",
       "0  [the_answer_is_if, answer_is_if_china, is_if_c...   \n",
       "1  [and_that_guy_is, that_guy_is_an, guy_is_an_ex...   \n",
       "2                                 [kennedi_is_a_gem]   \n",
       "3  [and_just_how_do, just_how_do_we, how_do_we_ge...   \n",
       "4  [that_man_was_go, man_was_go_for, was_go_for_a...   \n",
       "\n",
       "                              all_n_grams_lemmatized  \\\n",
       "0  [the, answer, be, if, china, and, india, dont,...   \n",
       "1  [and, that, guy, be, an, expert, be, screw, an...   \n",
       "2  [kennedy, be, a, gem, kennedy_be, be_a, a_gem,...   \n",
       "3  [and, just, how, do, we, get, a, nation, like,...   \n",
       "4  [that, man, be, go, for, an, oscar, that_man, ...   \n",
       "\n",
       "                                 all_n_grams_stemmed  \n",
       "0  [the, answer, is, if, china, and, india, dont,...  \n",
       "1  [and, that, guy, is, an, expert, were, screw, ...  \n",
       "2  [kennedi, is, a, gem, kennedi_is, is_a, a_gem,...  \n",
       "3  [and, just, how, do, we, get, a, nation, like,...  \n",
       "4  [that, man, was, go, for, an, oscar, that_man,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating one column with all n-grams (unigrams, bigrams, trigrams, fourgrams)\n",
    "processed_df[\"all_n_grams_lemmatized\"] = processed_df[\"lemmatized_tokens\"] + processed_df[\"bigrams_lemma\"] + processed_df[\"trigrams_lemma\"] + processed_df[\"fourgrams_lemma\"]\n",
    "processed_df[\"all_n_grams_stemmed\"] = processed_df[\"stemmed_tokens\"] + processed_df[\"bigrams_stem\"] + processed_df[\"trigrams_stem\"] + processed_df[\"fourgrams_stem\"]\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing 'climate': 14821\n"
     ]
    }
   ],
   "source": [
    "def contains_climate(lst):\n",
    "    return 'climate' in lst\n",
    "\n",
    "# Applying the function to create a boolean mask\n",
    "climate_mask = processed_df['all_n_grams_lemmatized'].apply(contains_climate)\n",
    "\n",
    "# Summing the rows where the mask is True\n",
    "sum_rows_with_climate = climate_mask.sum()\n",
    "\n",
    "print(f\"Number of rows containing 'climate': {sum_rows_with_climate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Dictionary\n",
    "\n",
    "Building the Dictionary for the Classifier. This process is based on the qualitative research and Word2Vec Model + Topic Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "7\n",
      "13\n",
      "5\n",
      "28\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Creating Dictionary \n",
    "\n",
    "sc1_kw = ['no_climate_emergency', 'melting', 'arctic_ice', 'arctic_sea ice', 'sea_level_rise', 'extreme_weather', 'global_cooling', 'greenland_ice',\n",
    "          'ice_cap', 'arctic_ice', 'extreme_heat', 'extreme_cold' ]\n",
    "# unsure &  included: 'melting'\n",
    "# unsure & not included: 'glacier', 'wildfires', 'climate emergency', 'unproven', 'global warming'\n",
    "\n",
    "sc2_kw = ['natural_cycle', 'CO2_is_not_the_cause', 'greenhouse_gas', 'no_CO2_Greenhouse_Effect', 'no_effect', 'miniscule_effect', 'Man_has_no_control']\n",
    "# unsure & not included: 'natural process'\n",
    "\n",
    "sc3_kw = ['plant_food', 'plant_growth', 'thrive', 'carbon_element_is_essential', 'average_temperature_increase', '1_degree', \n",
    "          'more_fossil_fuels', 'no_co2', 'plant_food', 'not_pollution', '0.1C', 'ppm', 'not_a_pollutant']\n",
    "# unsure & not included: 'beneficial'\n",
    "\n",
    "sc4_kw = ['green_energy', 'renewable_energy', 'energy_production', 'windmills', 'solar_panel']\n",
    "# unsure &  included: 'renewable energy'\n",
    "\n",
    "sc5_kw = ['alarmism', 'catastrophist', 'doomsday_cult', 'climate_hysteric', 'unscientific', 'corrupt_politician', 'LIE_ABOUT_EVERYTHING',\n",
    "          'idiocy', 'lunatics', 'CLIMATE_Worship', 'Climatists', 'alarmists', 'compliant_media', 'climate_hysteria', 'climate_narrative', 'climate_cult',\n",
    "             'scientism', 'climate_science_myths', 'lying_in_science', 'climate_apocalypse', 'propaganda', 'doomsayers', 'clown_show', 'fake_climate',\n",
    "               'climate_change_agenda', 'money_made', 'fake_news', 'climate_terrorists']\n",
    "# unsure & not included: 'scientist', 'global warming scam' (could also be sc7), 'greta', 'john kerry'\n",
    "\n",
    "sc7_kw = ['globalist', 'globalist_elites', 'elitist', 'global_government', 'one_world', 'one_world_government', 'globalism', \n",
    "          'one_world_utopia', 'new_world_order', 'enriching_themselves', 'saving_the_planet', 'control_over_your_lives', \n",
    "          'tyranny', 'global_elite', 'wef', 'population_control']\n",
    "# unsure & included: 'tyranny', 'population control'\n",
    "# unsure & not included: 'totalitarian'\n",
    "\n",
    "print(len(sc1_kw))\n",
    "print(len(sc2_kw))\n",
    "print(len(sc3_kw))\n",
    "print(len(sc4_kw))\n",
    "print(len(sc5_kw))\n",
    "print(len(sc7_kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc1_kw_lemmatized: ['no_climate_emergency', 'melt', 'arctic_ice', 'arctic_sea_ice', 'sea_level_rise', 'extreme_weather', 'global_cooling', 'greenland_ice', 'ice_cap', 'arctic_ice', 'extreme_heat', 'extreme_cold']\n",
      "sc2_kw_lemmatized: ['natural_cycle', 'co2_be_not_the_cause', 'greenhouse_gas', 'no_co2_greenhouse_effect', 'no_effect', 'miniscule_effect', 'man_have_no_control']\n",
      "sc3_kw_lemmatized: ['plant_food', 'plant_growth', 'thrive', 'carbon_element_be_essential', 'average_temperature_increase', '1_degree', 'more_fossil_fuel', 'no_co2', 'plant_food', 'not_pollution', '0.1c', 'ppm', 'not_a_pollutant']\n",
      "sc4_kw_lemmatized: ['green_energy', 'renewable_energy', 'energy_production', 'windmill', 'solar_panel']\n",
      "sc5_kw_lemmatized: ['alarmism', 'catastrophist', 'doomsday_cult', 'climate_hysteric', 'unscientific', 'corrupt_politician', 'lie_about_everything', 'idiocy', 'lunatic', 'climate_worship', 'climatists', 'alarmist', 'compliant_medium', 'climate_hysteria', 'climate_narrative', 'climate_cult', 'scientism', 'climate_science_myth', 'lie_in_science', 'climate_apocalypse', 'propaganda', 'doomsayers', 'clown_show', 'fake_climate', 'climate_change_agenda', 'money_make', 'fake_news', 'climate_terrorist']\n",
      "sc7_kw_lemmatized: ['globalist', 'globalist_elite', 'elitist', 'global_government', 'one_world', 'one_world_government', 'globalism', 'one_world_utopia', 'new_world_order', 'enrich_themselves', 'save_the_planet', 'control_over_your_life', 'tyranny', 'global_elite', 'wef', 'population_control']\n",
      "sc1_kw_stemmed: ['no_climat_emerg', 'melt', 'arctic_ice', 'arctic_sea_ice', 'sea_level_rise', 'extrem_weather', 'global_cool', 'greenland_ice', 'ice_cap', 'arctic_ice', 'extrem_heat', 'extrem_cold']\n",
      "sc2_kw_stemmed: ['natur_cycl', 'co2_is_not_the_caus', 'greenhous_gas', 'no_co2_greenhous_effect', 'no_effect', 'miniscul_effect', 'man_has_no_control']\n",
      "sc3_kw_stemmed: ['plant_food', 'plant_growth', 'thrive', 'carbon_element_is_essenti', 'averag_temperatur_increas', '1_degre', 'more_fossil_fuel', 'no_co2', 'plant_food', 'not_pollut', '0.1c', 'ppm', 'not_a_pollut']\n",
      "sc4_kw_stemmed: ['green_energi', 'renew_energi', 'energi_product', 'windmil', 'solar_panel']\n",
      "sc5_kw_stemmed: ['alarm', 'catastrophist', 'doomsday_cult', 'climat_hyster', 'unscientif', 'corrupt_politician', 'lie_about_everyth', 'idioci', 'lunat', 'climat_worship', 'climatist', 'alarmist', 'compliant_media', 'climat_hysteria', 'climat_narrat', 'climat_cult', 'scientism', 'climat_scienc_myth', 'lie_in_scienc', 'climat_apocalyps', 'propaganda', 'doomsay', 'clown_show', 'fake_climat', 'climat_chang_agenda', 'money_made', 'fake_news', 'climat_terrorist']\n",
      "sc7_kw_stemmed: ['globalist', 'globalist_elit', 'elitist', 'global_govern', 'one_world', 'one_world_govern', 'global', 'one_world_utopia', 'new_world_order', 'enrich_themselv', 'save_the_planet', 'control_over_your_live', 'tyranni', 'global_elit', 'wef', 'popul_control']\n"
     ]
    }
   ],
   "source": [
    "# lowercase and stemming/lemmatizing the keyword lists \n",
    "\n",
    "# Function to remove underscores and convert to lowercase\n",
    "def preprocess_keywords(keywords):\n",
    "    return [keyword.replace('_', ' ').lower() for keyword in keywords]\n",
    "\n",
    "# Stem words\n",
    "def stem_words(words):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return [\" \".join([stemmer.stem(word) for word in word_tokenize(keyword)]) for keyword in words]\n",
    "\n",
    "# Lemmatize words\n",
    "def lemmatize_words(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Mapping NLTK POS tags to WordNet POS tags\n",
    "    tag_map = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'N': wordnet.NOUN\n",
    "    }\n",
    "    \n",
    "    lemmatized_keywords = []\n",
    "    for keyword in words:\n",
    "        tokens = word_tokenize(keyword)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, tag_map.get(tag[0], wordnet.NOUN)) for word, tag in pos_tags]\n",
    "        lemmatized_keywords.append(\" \".join(lemmatized_tokens))\n",
    "    \n",
    "    return lemmatized_keywords\n",
    "\n",
    "\n",
    "# Replacing the whitespaces with underscores again to create n-grams\n",
    "def postprocess_keywords(keywords):\n",
    "    return [keyword.replace(' ', '_') for keyword in keywords]\n",
    "\n",
    "# Apply the preprocessing, stemming, and lemmatization\n",
    "sc1_kw_lemmatized = postprocess_keywords(lemmatize_words(preprocess_keywords(sc1_kw)))\n",
    "sc2_kw_lemmatized = postprocess_keywords(lemmatize_words(preprocess_keywords(sc2_kw)))\n",
    "sc3_kw_lemmatized = postprocess_keywords(lemmatize_words(preprocess_keywords(sc3_kw)))\n",
    "sc4_kw_lemmatized = postprocess_keywords(lemmatize_words(preprocess_keywords(sc4_kw)))\n",
    "sc5_kw_lemmatized = postprocess_keywords(lemmatize_words(preprocess_keywords(sc5_kw)))\n",
    "sc7_kw_lemmatized = postprocess_keywords(lemmatize_words(preprocess_keywords(sc7_kw)))\n",
    "\n",
    "sc1_kw_stemmed = postprocess_keywords(stem_words(preprocess_keywords(sc1_kw)))\n",
    "sc2_kw_stemmed = postprocess_keywords(stem_words(preprocess_keywords(sc2_kw)))\n",
    "sc3_kw_stemmed = postprocess_keywords(stem_words(preprocess_keywords(sc3_kw)))\n",
    "sc4_kw_stemmed = postprocess_keywords(stem_words(preprocess_keywords(sc4_kw)))\n",
    "sc5_kw_stemmed = postprocess_keywords(stem_words(preprocess_keywords(sc5_kw)))\n",
    "sc7_kw_stemmed = postprocess_keywords(stem_words(preprocess_keywords(sc7_kw)))\n",
    "\n",
    "# print lemmatized dictionary\n",
    "print(\"sc1_kw_lemmatized:\", sc1_kw_lemmatized)\n",
    "print(\"sc2_kw_lemmatized:\", sc2_kw_lemmatized)\n",
    "print(\"sc3_kw_lemmatized:\", sc3_kw_lemmatized)\n",
    "print(\"sc4_kw_lemmatized:\", sc4_kw_lemmatized)\n",
    "print(\"sc5_kw_lemmatized:\", sc5_kw_lemmatized)\n",
    "print(\"sc7_kw_lemmatized:\", sc7_kw_lemmatized)\n",
    "\n",
    "# print stemmed dictionary\n",
    "print(\"sc1_kw_stemmed:\", sc1_kw_stemmed)\n",
    "print(\"sc2_kw_stemmed:\", sc2_kw_stemmed)\n",
    "print(\"sc3_kw_stemmed:\", sc3_kw_stemmed)\n",
    "print(\"sc4_kw_stemmed:\", sc4_kw_stemmed)\n",
    "print(\"sc5_kw_stemmed:\", sc5_kw_stemmed)\n",
    "print(\"sc7_kw_stemmed:\", sc7_kw_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['no_climate_emergency', 'melt', 'arctic_ice', 'arctic_sea_ice', 'sea_level_rise', 'extreme_weather', 'global_cooling', 'greenland_ice', 'ice_cap', 'arctic_ice', 'extreme_heat', 'extreme_cold'], 2: ['natural_cycle', 'co2_be_not_the_cause', 'greenhouse_gas', 'no_co2_greenhouse_effect', 'no_effect', 'miniscule_effect', 'man_have_no_control'], 3: ['plant_food', 'plant_growth', 'thrive', 'carbon_element_be_essential', 'average_temperature_increase', '1_degree', 'more_fossil_fuel', 'no_co2', 'plant_food', 'not_pollution', '0.1c', 'ppm', 'not_a_pollutant'], 4: ['green_energy', 'renewable_energy', 'energy_production', 'windmill', 'solar_panel'], 5: ['alarmism', 'catastrophist', 'doomsday_cult', 'climate_hysteric', 'unscientific', 'corrupt_politician', 'lie_about_everything', 'idiocy', 'lunatic', 'climate_worship', 'climatists', 'alarmist', 'compliant_medium', 'climate_hysteria', 'climate_narrative', 'climate_cult', 'scientism', 'climate_science_myth', 'lie_in_science', 'climate_apocalypse', 'propaganda', 'doomsayers', 'clown_show', 'fake_climate', 'climate_change_agenda', 'money_make', 'fake_news', 'climate_terrorist'], 6: ['globalist', 'globalist_elite', 'elitist', 'global_government', 'one_world', 'one_world_government', 'globalism', 'one_world_utopia', 'new_world_order', 'enrich_themselves', 'save_the_planet', 'control_over_your_life', 'tyranny', 'global_elite', 'wef', 'population_control']}\n",
      "{1: ['no_climat_emerg', 'melt', 'arctic_ice', 'arctic_sea_ice', 'sea_level_rise', 'extrem_weather', 'global_cool', 'greenland_ice', 'ice_cap', 'arctic_ice', 'extrem_heat', 'extrem_cold'], 2: ['natur_cycl', 'co2_is_not_the_caus', 'greenhous_gas', 'no_co2_greenhous_effect', 'no_effect', 'miniscul_effect', 'man_has_no_control'], 3: ['plant_food', 'plant_growth', 'thrive', 'carbon_element_is_essenti', 'averag_temperatur_increas', '1_degre', 'more_fossil_fuel', 'no_co2', 'plant_food', 'not_pollut', '0.1c', 'ppm', 'not_a_pollut'], 4: ['green_energi', 'renew_energi', 'energi_product', 'windmil', 'solar_panel'], 5: ['alarm', 'catastrophist', 'doomsday_cult', 'climat_hyster', 'unscientif', 'corrupt_politician', 'lie_about_everyth', 'idioci', 'lunat', 'climat_worship', 'climatist', 'alarmist', 'compliant_media', 'climat_hysteria', 'climat_narrat', 'climat_cult', 'scientism', 'climat_scienc_myth', 'lie_in_scienc', 'climat_apocalyps', 'propaganda', 'doomsay', 'clown_show', 'fake_climat', 'climat_chang_agenda', 'money_made', 'fake_news', 'climat_terrorist'], 6: ['globalist', 'globalist_elit', 'elitist', 'global_govern', 'one_world', 'one_world_govern', 'global', 'one_world_utopia', 'new_world_order', 'enrich_themselv', 'save_the_planet', 'control_over_your_live', 'tyranni', 'global_elit', 'wef', 'popul_control']}\n"
     ]
    }
   ],
   "source": [
    "# Creating the dictionaries for the classifier\n",
    "keyword_dict_lemmatized = {\n",
    "    1: sc1_kw_lemmatized,\n",
    "    2: sc2_kw_lemmatized,\n",
    "    3: sc3_kw_lemmatized,\n",
    "    4: sc4_kw_lemmatized,\n",
    "    5: sc5_kw_lemmatized,\n",
    "    6: sc7_kw_lemmatized\n",
    "}\n",
    "\n",
    "\n",
    "keyword_dict_stemmed = {\n",
    "    1: sc1_kw_stemmed,\n",
    "    2: sc2_kw_stemmed,\n",
    "    3: sc3_kw_stemmed,\n",
    "    4: sc4_kw_stemmed,\n",
    "    5: sc5_kw_stemmed,\n",
    "    6: sc7_kw_stemmed\n",
    "}\n",
    "\n",
    "keyword_dict_lemmatized_valid = {\n",
    "    1: ['climate'],\n",
    "    2: ['climate_change']\n",
    "}\n",
    "\n",
    "#print keyword dictionaries\n",
    "print(keyword_dict_lemmatized)\n",
    "print(keyword_dict_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "Classification of YouTube comments related to each claim, based on the text of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>bigrams_lemma</th>\n",
       "      <th>trigrams_lemma</th>\n",
       "      <th>fourgrams_lemma</th>\n",
       "      <th>bigrams_stem</th>\n",
       "      <th>trigrams_stem</th>\n",
       "      <th>fourgrams_stem</th>\n",
       "      <th>all_n_grams_lemmatized</th>\n",
       "      <th>all_n_grams_stemmed</th>\n",
       "      <th>category_lemmatized_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>1</td>\n",
       "      <td>the answer is if china and india dont help it ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the answer is if china and india dont help it ...</td>\n",
       "      <td>the answer be if china and india dont help it ...</td>\n",
       "      <td>[the, answer, is, if, china, and, india, dont,...</td>\n",
       "      <td>[the, answer, be, if, china, and, india, dont,...</td>\n",
       "      <td>[the_answer, answer_be, be_if, if_china, china...</td>\n",
       "      <td>[the_answer_be, answer_be_if, be_if_china, if_...</td>\n",
       "      <td>[the_answer_be_if, answer_be_if_china, be_if_c...</td>\n",
       "      <td>[the_answer, answer_is, is_if, if_china, china...</td>\n",
       "      <td>[the_answer_is, answer_is_if, is_if_china, if_...</td>\n",
       "      <td>[the_answer_is_if, answer_is_if_china, is_if_c...</td>\n",
       "      <td>[the, answer, be, if, china, and, india, dont,...</td>\n",
       "      <td>[the, answer, is, if, china, and, india, dont,...</td>\n",
       "      <td>[uncategorized]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>2</td>\n",
       "      <td>and that guy is an expert were screwed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>and that guy is an expert were screw</td>\n",
       "      <td>and that guy be an expert be screw</td>\n",
       "      <td>[and, that, guy, is, an, expert, were, screw]</td>\n",
       "      <td>[and, that, guy, be, an, expert, be, screw]</td>\n",
       "      <td>[and_that, that_guy, guy_be, be_an, an_expert,...</td>\n",
       "      <td>[and_that_guy, that_guy_be, guy_be_an, be_an_e...</td>\n",
       "      <td>[and_that_guy_be, that_guy_be_an, guy_be_an_ex...</td>\n",
       "      <td>[and_that, that_guy, guy_is, is_an, an_expert,...</td>\n",
       "      <td>[and_that_guy, that_guy_is, guy_is_an, is_an_e...</td>\n",
       "      <td>[and_that_guy_is, that_guy_is_an, guy_is_an_ex...</td>\n",
       "      <td>[and, that, guy, be, an, expert, be, screw, an...</td>\n",
       "      <td>[and, that, guy, is, an, expert, were, screw, ...</td>\n",
       "      <td>[uncategorized]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>4</td>\n",
       "      <td>kennedy is a gem</td>\n",
       "      <td>2.0</td>\n",
       "      <td>kennedi is a gem</td>\n",
       "      <td>kennedy be a gem</td>\n",
       "      <td>[kennedi, is, a, gem]</td>\n",
       "      <td>[kennedy, be, a, gem]</td>\n",
       "      <td>[kennedy_be, be_a, a_gem]</td>\n",
       "      <td>[kennedy_be_a, be_a_gem]</td>\n",
       "      <td>[kennedy_be_a_gem]</td>\n",
       "      <td>[kennedi_is, is_a, a_gem]</td>\n",
       "      <td>[kennedi_is_a, is_a_gem]</td>\n",
       "      <td>[kennedi_is_a_gem]</td>\n",
       "      <td>[kennedy, be, a, gem, kennedy_be, be_a, a_gem,...</td>\n",
       "      <td>[kennedi, is, a, gem, kennedi_is, is_a, a_gem,...</td>\n",
       "      <td>[uncategorized]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>0</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>and just how do we get a nation like china to ...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "      <td>[and_just, just_how, how_do, do_we, we_get, ge...</td>\n",
       "      <td>[and_just_how, just_how_do, how_do_we, do_we_g...</td>\n",
       "      <td>[and_just_how_do, just_how_do_we, how_do_we_ge...</td>\n",
       "      <td>[and_just, just_how, how_do, do_we, we_get, ge...</td>\n",
       "      <td>[and_just_how, just_how_do, how_do_we, do_we_g...</td>\n",
       "      <td>[and_just_how_do, just_how_do_we, how_do_we_ge...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "      <td>[and, just, how, do, we, get, a, nation, like,...</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>that man was going for an oscar</td>\n",
       "      <td>4.0</td>\n",
       "      <td>that man was go for an oscar</td>\n",
       "      <td>that man be go for an oscar</td>\n",
       "      <td>[that, man, was, go, for, an, oscar]</td>\n",
       "      <td>[that, man, be, go, for, an, oscar]</td>\n",
       "      <td>[that_man, man_be, be_go, go_for, for_an, an_o...</td>\n",
       "      <td>[that_man_be, man_be_go, be_go_for, go_for_an,...</td>\n",
       "      <td>[that_man_be_go, man_be_go_for, be_go_for_an, ...</td>\n",
       "      <td>[that_man, man_was, was_go, go_for, for_an, an...</td>\n",
       "      <td>[that_man_was, man_was_go, was_go_for, go_for_...</td>\n",
       "      <td>[that_man_was_go, man_was_go_for, was_go_for_a...</td>\n",
       "      <td>[that, man, be, go, for, an, oscar, that_man, ...</td>\n",
       "      <td>[that, man, was, go, for, an, oscar, that_man,...</td>\n",
       "      <td>[uncategorized]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id published_at  like_count  \\\n",
       "0  uW6fi2tCnAc   2023-02-19           1   \n",
       "1  uW6fi2tCnAc   2023-02-19           2   \n",
       "2  uW6fi2tCnAc   2023-02-18           4   \n",
       "3  uW6fi2tCnAc   2023-02-18           0   \n",
       "4  uW6fi2tCnAc   2023-02-18           3   \n",
       "\n",
       "                                                text  author  \\\n",
       "0  the answer is if china and india dont help it ...     0.0   \n",
       "1             and that guy is an expert were screwed     1.0   \n",
       "2                                   kennedy is a gem     2.0   \n",
       "3  and just how do we get a nation like china to ...     3.0   \n",
       "4                    that man was going for an oscar     4.0   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  the answer is if china and india dont help it ...   \n",
       "1               and that guy is an expert were screw   \n",
       "2                                   kennedi is a gem   \n",
       "3  and just how do we get a nation like china to ...   \n",
       "4                       that man was go for an oscar   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  the answer be if china and india dont help it ...   \n",
       "1                 and that guy be an expert be screw   \n",
       "2                                   kennedy be a gem   \n",
       "3  and just how do we get a nation like china to ...   \n",
       "4                        that man be go for an oscar   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [the, answer, is, if, china, and, india, dont,...   \n",
       "1      [and, that, guy, is, an, expert, were, screw]   \n",
       "2                              [kennedi, is, a, gem]   \n",
       "3  [and, just, how, do, we, get, a, nation, like,...   \n",
       "4               [that, man, was, go, for, an, oscar]   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [the, answer, be, if, china, and, india, dont,...   \n",
       "1        [and, that, guy, be, an, expert, be, screw]   \n",
       "2                              [kennedy, be, a, gem]   \n",
       "3  [and, just, how, do, we, get, a, nation, like,...   \n",
       "4                [that, man, be, go, for, an, oscar]   \n",
       "\n",
       "                                       bigrams_lemma  \\\n",
       "0  [the_answer, answer_be, be_if, if_china, china...   \n",
       "1  [and_that, that_guy, guy_be, be_an, an_expert,...   \n",
       "2                          [kennedy_be, be_a, a_gem]   \n",
       "3  [and_just, just_how, how_do, do_we, we_get, ge...   \n",
       "4  [that_man, man_be, be_go, go_for, for_an, an_o...   \n",
       "\n",
       "                                      trigrams_lemma  \\\n",
       "0  [the_answer_be, answer_be_if, be_if_china, if_...   \n",
       "1  [and_that_guy, that_guy_be, guy_be_an, be_an_e...   \n",
       "2                           [kennedy_be_a, be_a_gem]   \n",
       "3  [and_just_how, just_how_do, how_do_we, do_we_g...   \n",
       "4  [that_man_be, man_be_go, be_go_for, go_for_an,...   \n",
       "\n",
       "                                     fourgrams_lemma  \\\n",
       "0  [the_answer_be_if, answer_be_if_china, be_if_c...   \n",
       "1  [and_that_guy_be, that_guy_be_an, guy_be_an_ex...   \n",
       "2                                 [kennedy_be_a_gem]   \n",
       "3  [and_just_how_do, just_how_do_we, how_do_we_ge...   \n",
       "4  [that_man_be_go, man_be_go_for, be_go_for_an, ...   \n",
       "\n",
       "                                        bigrams_stem  \\\n",
       "0  [the_answer, answer_is, is_if, if_china, china...   \n",
       "1  [and_that, that_guy, guy_is, is_an, an_expert,...   \n",
       "2                          [kennedi_is, is_a, a_gem]   \n",
       "3  [and_just, just_how, how_do, do_we, we_get, ge...   \n",
       "4  [that_man, man_was, was_go, go_for, for_an, an...   \n",
       "\n",
       "                                       trigrams_stem  \\\n",
       "0  [the_answer_is, answer_is_if, is_if_china, if_...   \n",
       "1  [and_that_guy, that_guy_is, guy_is_an, is_an_e...   \n",
       "2                           [kennedi_is_a, is_a_gem]   \n",
       "3  [and_just_how, just_how_do, how_do_we, do_we_g...   \n",
       "4  [that_man_was, man_was_go, was_go_for, go_for_...   \n",
       "\n",
       "                                      fourgrams_stem  \\\n",
       "0  [the_answer_is_if, answer_is_if_china, is_if_c...   \n",
       "1  [and_that_guy_is, that_guy_is_an, guy_is_an_ex...   \n",
       "2                                 [kennedi_is_a_gem]   \n",
       "3  [and_just_how_do, just_how_do_we, how_do_we_ge...   \n",
       "4  [that_man_was_go, man_was_go_for, was_go_for_a...   \n",
       "\n",
       "                              all_n_grams_lemmatized  \\\n",
       "0  [the, answer, be, if, china, and, india, dont,...   \n",
       "1  [and, that, guy, be, an, expert, be, screw, an...   \n",
       "2  [kennedy, be, a, gem, kennedy_be, be_a, a_gem,...   \n",
       "3  [and, just, how, do, we, get, a, nation, like,...   \n",
       "4  [that, man, be, go, for, an, oscar, that_man, ...   \n",
       "\n",
       "                                 all_n_grams_stemmed  \\\n",
       "0  [the, answer, is, if, china, and, india, dont,...   \n",
       "1  [and, that, guy, is, an, expert, were, screw, ...   \n",
       "2  [kennedi, is, a, gem, kennedi_is, is_a, a_gem,...   \n",
       "3  [and, just, how, do, we, get, a, nation, like,...   \n",
       "4  [that, man, was, go, for, an, oscar, that_man,...   \n",
       "\n",
       "  category_lemmatized_comments  \n",
       "0              [uncategorized]  \n",
       "1              [uncategorized]  \n",
       "2              [uncategorized]  \n",
       "3                          [6]  \n",
       "4              [uncategorized]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifiying the comments into categories\n",
    "def classify_comments(comments, keyword_dict):\n",
    "    classifications = [] #initialize empty list of classifications\n",
    "    \n",
    "    for comment in comments: #loop through each comment of the df \n",
    "        categories = [] #initialize empty list of categories\n",
    "        comment_str = \",\".join(comment)  # Join the tokens of one comement into a single string for easier matching\n",
    "        \n",
    "        for category, keywords in keyword_dict.items(): #iterating through each key-value pair of the dictionary\n",
    "            for keyword in keywords: #for each category: iterate through list of keywords. Check if each keyword is present in comment_str\n",
    "                if keyword in comment_str:\n",
    "                    categories.append(category) #if a keyworrd is found, the category is appended to list.\n",
    "                    break  # Stop checking more keywords for this category\n",
    "        \n",
    "        if not categories:\n",
    "            categories = ['uncategorized']\n",
    "        \n",
    "        classifications.append(categories)\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "# Apply classifier to lemmatized comments \n",
    "processed_df['category_lemmatized_comments'] = classify_comments(processed_df['all_n_grams_lemmatized'], keyword_dict_lemmatized)\n",
    "processed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_lemmatized_comments\n",
       "[uncategorized]    88626\n",
       "[5]                 2957\n",
       "[6]                 1891\n",
       "[1]                 1083\n",
       "[4]                  580\n",
       "[3]                  571\n",
       "[2]                  296\n",
       "[5, 6]               141\n",
       "[1, 5]                72\n",
       "[3, 5]                51\n",
       "[1, 3]                42\n",
       "[2, 3]                38\n",
       "[3, 6]                30\n",
       "[4, 5]                28\n",
       "[1, 6]                28\n",
       "[1, 2]                25\n",
       "[2, 5]                18\n",
       "[1, 4]                18\n",
       "[4, 6]                16\n",
       "[3, 4]                11\n",
       "[2, 4]                10\n",
       "[1, 3, 5]              9\n",
       "[1, 2, 3, 5]           7\n",
       "[2, 6]                 7\n",
       "[1, 5, 6]              5\n",
       "[1, 2, 3]              5\n",
       "[2, 3, 5]              5\n",
       "[1, 2, 5]              4\n",
       "[2, 3, 5, 6]           4\n",
       "[1, 3, 6]              3\n",
       "[1, 3, 4, 6]           3\n",
       "[1, 2, 6]              2\n",
       "[2, 5, 6]              1\n",
       "[1, 2, 4]              1\n",
       "[3, 5, 6]              1\n",
       "[1, 2, 3, 4, 5]        1\n",
       "[2, 3, 4]              1\n",
       "[1, 3, 4, 5]           1\n",
       "[1, 2, 3, 5, 6]        1\n",
       "[3, 4, 6]              1\n",
       "[2, 4, 5]              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categories Count of Lemmatized Unigrams\n",
    "processed_df['category_lemmatized_comments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_stemmed_comments\n",
      "[uncategorized]       86049\n",
      "[6]                    4312\n",
      "[5]                    2913\n",
      "[1]                     807\n",
      "[4]                     540\n",
      "[3]                     522\n",
      "[5, 6]                  339\n",
      "[1, 6]                  282\n",
      "[2]                     226\n",
      "[3, 6]                   82\n",
      "[2, 6]                   79\n",
      "[1, 5]                   58\n",
      "[4, 6]                   51\n",
      "[3, 5]                   43\n",
      "[1, 5, 6]                32\n",
      "[4, 5]                   23\n",
      "[1, 3]                   23\n",
      "[1, 3, 6]                22\n",
      "[2, 3, 6]                22\n",
      "[2, 3]                   17\n",
      "[1, 2]                   17\n",
      "[2, 5]                   16\n",
      "[3, 5, 6]                13\n",
      "[1, 4]                   12\n",
      "[3, 4]                   11\n",
      "[1, 2, 6]                 8\n",
      "[2, 4]                    8\n",
      "[1, 3, 5, 6]              7\n",
      "[2, 3, 5, 6]              6\n",
      "[4, 5, 6]                 6\n",
      "[1, 2, 3, 5]              5\n",
      "[1, 2, 5, 6]              5\n",
      "[2, 5, 6]                 5\n",
      "[1, 2, 3]                 4\n",
      "[1, 3, 4, 6]              3\n",
      "[3, 4, 6]                 3\n",
      "[2, 3, 5]                 3\n",
      "[1, 4, 6]                 3\n",
      "[1, 3, 5]                 3\n",
      "[1, 2, 3, 5, 6]           3\n",
      "[2, 4, 6]                 2\n",
      "[1, 2, 4, 6]              1\n",
      "[2, 3, 4, 6]              1\n",
      "[1, 2, 5]                 1\n",
      "[1, 2, 3, 6]              1\n",
      "[1, 3, 4, 5, 6]           1\n",
      "[1, 2, 3, 4, 5, 6]        1\n",
      "[1, 4, 5]                 1\n",
      "[1, 3, 4]                 1\n",
      "[1, 4, 5, 6]              1\n",
      "[2, 4, 5, 6]              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Categories of Stemmed Unigrams\n",
    "processed_df['category_stemmed_comments'] = classify_comments(processed_df['all_n_grams_stemmed'], keyword_dict_stemmed)\n",
    "print(processed_df['category_stemmed_comments'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_lemmatized_comments_validated\n",
      "[uncategorized]    81518\n",
      "[1]                 7795\n",
      "[1, 2]              7282\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Categories of Validation with Climate and Climate Change\n",
    "processed_df['category_lemmatized_comments_validated'] = classify_comments(processed_df['all_n_grams_lemmatized'], keyword_dict_lemmatized_valid)\n",
    "print(processed_df['category_lemmatized_comments_validated'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Methods - Topic Modelling\n",
    "_____\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Libraries](#libraries)\n",
    "2. [Data Preprocessing](#data-preprocessing)\n",
    "3. [Topic Modelling](#topic-modelling)\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "All libraries which are needed to execute the code are listed here. Install the packages by using the `requirements.txt` file. \n",
    "\n",
    "The documentation can be found in the [README.md](README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tqdm import tqdm \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, LsiModel, HdpModel\n",
    "\n",
    "# import functions\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "df = pd.read_csv('comments.csv', index_col=0)\n",
    "\n",
    "#set pandas option to show more text\n",
    "pd.set_option('display.max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data with using functions from functions.py\n",
    "processed_df = (\n",
    "    df.pipe(remove_users, 'text')\n",
    "      .pipe(lowercase_text, 'text')\n",
    "      .pipe(remove_whitespace, 'text')\n",
    "      .pipe(remove_stopwords, 'text')\n",
    "      .pipe(remove_punctuation, 'text')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text column to string\n",
    "processed_df['text'] = processed_df['text'].astype('str')\n",
    "processed_df['text'] = processed_df['text'].str.replace('\\'', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stemming to reduce words to their root words\n",
    "processed_df = stem_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lemmatization to reduce words to their root form\n",
    "processed_df = lemmatize_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date format\n",
    "processed_df = convert_date_format(processed_df, 'published_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7333, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7333/7333 [00:05<00:00, 1226.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Replacing NaN-values and aggregating data by date\n",
    "processed_df.lemmatized_text = processed_df.lemmatized_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "processed_df.stemmed_text = processed_df.stemmed_text.apply(lambda x: '' if str(x) == 'nan' else x)\n",
    "\n",
    "\n",
    "# aggregating tweet data by dates and affiliation \n",
    "df_agg = processed_df.groupby(['published_at', 'video_id'], as_index = False).agg({'text': ' '.join, \n",
    "                                                                            'lemmatized_text': ' '.join,\n",
    "                                                                            'stemmed_text': ' '.join})\n",
    "# checking dimensions of new dataset and viewing the dataset\n",
    "print(df_agg.shape)\n",
    "\n",
    "#Defining NLTK's TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# tokenizing and creating a column of unigrams from the stemmed tweet text. \n",
    "df_agg['unigrams'] = df_agg['stemmed_text'].progress_apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7333/7333 [00:00<00:00, 27452.09it/s]\n"
     ]
    }
   ],
   "source": [
    "#Creating a column with bigrams by applying function to column of unigrams\n",
    "df_agg['bigrams'] = df_agg.unigrams.progress_apply(lambda x: bigrams(x))\n",
    "df_agg['tokens'] = df_agg.unigrams+df_agg.bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928879\n"
     ]
    }
   ],
   "source": [
    "# insert the column where you saved unigram and bigram tokens between the parentheses\n",
    "id2word = Dictionary(df_agg['tokens']) \n",
    "\n",
    "# viewing how many words are in our vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19559\n"
     ]
    }
   ],
   "source": [
    "# removing very frequent and infrequent words\n",
    "id2word.filter_extremes(no_below=10, \n",
    "                        no_above=.999,\n",
    "                        keep_n=None) \n",
    "\n",
    "# viewing how many words are now in our vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating corpus\n",
    "corpus = [id2word.doc2bow(doc) for doc in df_agg['tokens']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "___\n",
    "\n",
    "\n",
    "- using topic modelling to explore for keywords.\n",
    "- using LSI, HDP, and LDA to get an impression on the topics of our observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.386*\"peopl\" + 0.305*\"â€™\" + 0.229*\"govern\" + 0.184*\"land\" + 0.165*\"fire\" + 0.161*\"like\" + 0.155*\"get\" + 0.128*\"go\" + 0.121*\"climat\" + 0.117*\"need\"'),\n",
       " (1,\n",
       "  '0.507*\"climat\" + 0.341*\"chang\" + 0.264*\"climat_chang\" + -0.186*\"land\" + -0.174*\"govern\" + -0.157*\"fire\" + -0.142*\"peopl\" + 0.131*\"year\" + -0.126*\"maui\" + -0.123*\"tulsi\"'),\n",
       " (2,\n",
       "  '0.287*\"kerri\" + -0.259*\"chang\" + 0.222*\"ðŸ˜‚\" + 0.210*\"fuel\" + -0.209*\"climat\" + -0.197*\"climat_chang\" + 0.177*\"fossil\" + 0.168*\"lie\" + 0.159*\"fossil_fuel\" + 0.151*\"like\"'),\n",
       " (3,\n",
       "  '-0.428*\"kerri\" + 0.304*\"fuel\" + 0.279*\"fossil\" + 0.252*\"fossil_fuel\" + -0.211*\"john\" + -0.172*\"lie\" + -0.171*\"john_kerri\" + -0.159*\"fli\" + 0.152*\"oil\" + 0.136*\"energi\"'),\n",
       " (4,\n",
       "  '0.340*\"ðŸ˜‚\" + 0.265*\"expert\" + 0.202*\"kid\" + 0.190*\"chang\" + -0.168*\"kerri\" + -0.160*\"thank\" + 0.156*\"ðŸ˜‚_ðŸ˜‚\" + -0.153*\"toni\" + 0.141*\"climat_chang\" + 0.140*\"greta\"'),\n",
       " (5,\n",
       "  '0.264*\"greta\" + 0.204*\"expert\" + 0.186*\"know\" + 0.177*\"co2\" + -0.168*\"climat_chang\" + -0.167*\"chang\" + -0.164*\"climat\" + 0.159*\"toni\" + -0.135*\"fuel\" + 0.129*\"kid\"'),\n",
       " (6,\n",
       "  '0.347*\"cloud\" + 0.319*\"seed\" + 0.257*\"cloud_seed\" + 0.210*\"weather\" + 0.150*\"rain\" + -0.132*\"expert\" + -0.116*\"money\" + -0.116*\"chang\" + 0.112*\"sun\" + 0.110*\"year\"'),\n",
       " (7,\n",
       "  '0.534*\"greta\" + -0.202*\"co2\" + -0.170*\"expert\" + -0.140*\"toni\" + -0.129*\"year\" + 0.125*\"russia\" + 0.118*\"climat_chang\" + 0.116*\"uranium\" + -0.115*\"temperatur\" + 0.114*\"nuclear\"'),\n",
       " (8,\n",
       "  '0.337*\"greta\" + 0.293*\"toni\" + -0.282*\"cloud\" + -0.276*\"seed\" + -0.222*\"cloud_seed\" + -0.163*\"â€™\" + 0.128*\"lie\" + 0.122*\"thank\" + 0.110*\"warm\" + -0.109*\"peterson\"'),\n",
       " (9,\n",
       "  '-0.473*\"toni\" + 0.166*\"greta\" + -0.157*\"thank\" + 0.150*\"earth\" + -0.147*\"climat\" + 0.140*\"co2\" + -0.138*\"thank_toni\" + 0.132*\"sun\" + -0.129*\"seed\" + 0.128*\"wef\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating LSI Model\n",
    "lsi_model = LsiModel(corpus=corpus, num_topics=10, id2word=id2word)\n",
    "lsi_model.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*climat + 0.008*â€™ + 0.008*peopl + 0.007*chang + 0.005*like + 0.005*climat_chang + 0.005*year + 0.004*get + 0.004*one + 0.004*would + 0.004*go + 0.003*know + 0.003*us + 0.003*need + 0.003*say + 0.003*co2 + 0.003*time + 0.003*use + 0.003*make + 0.003*thank'),\n",
       " (1,\n",
       "  '0.009*climat + 0.005*peopl + 0.005*chang + 0.005*â€™ + 0.004*one + 0.004*like + 0.004*would + 0.004*climat_chang + 0.003*world + 0.003*year + 0.003*energi + 0.003*co2 + 0.003*get + 0.003*make + 0.003*thank + 0.003*know + 0.003*scienc + 0.003*use + 0.003*go + 0.003*us'),\n",
       " (2,\n",
       "  '0.011*climat + 0.006*chang + 0.005*â€™ + 0.004*climat_chang + 0.004*peopl + 0.004*like + 0.004*year + 0.003*would + 0.003*one + 0.003*know + 0.003*thank + 0.003*go + 0.003*say + 0.003*time + 0.003*get + 0.002*world + 0.002*scienc + 0.002*think + 0.002*dr + 0.002*warm'),\n",
       " (3,\n",
       "  '0.008*climat + 0.005*chang + 0.004*climat_chang + 0.004*â€™ + 0.004*peopl + 0.003*like + 0.003*year + 0.003*would + 0.003*get + 0.003*thank + 0.003*go + 0.002*one + 0.002*need + 0.002*co2 + 0.002*energi + 0.002*scienc + 0.002*make + 0.002*time + 0.002*think + 0.002*say'),\n",
       " (4,\n",
       "  '0.008*climat + 0.005*chang + 0.004*â€™ + 0.004*year + 0.004*like + 0.003*climat_chang + 0.003*peopl + 0.003*would + 0.003*one + 0.003*thank + 0.002*get + 0.002*warm + 0.002*co2 + 0.002*global + 0.002*scienc + 0.002*much + 0.002*time + 0.002*need + 0.002*great + 0.002*think'),\n",
       " (5,\n",
       "  '0.007*climat + 0.004*chang + 0.003*â€™ + 0.003*peopl + 0.003*would + 0.003*thank + 0.003*year + 0.003*like + 0.003*one + 0.002*dr + 0.002*climat_chang + 0.002*global + 0.002*co2 + 0.002*world + 0.002*scienc + 0.002*use + 0.002*scientist + 0.002*time + 0.002*go + 0.002*make'),\n",
       " (6,\n",
       "  '0.004*climat + 0.003*lindzen + 0.002*chang + 0.002*scienc + 0.002*would + 0.002*thank + 0.002*â€™ + 0.002*warm + 0.002*model + 0.002*earth + 0.002*climat_chang + 0.002*scientist + 0.002*peopl + 0.002*one + 0.002*much + 0.002*get + 0.001*dr + 0.001*like + 0.001*jordan + 0.001*think'),\n",
       " (7,\n",
       "  '0.003*climat + 0.003*â€™ + 0.002*normal + 0.002*greta + 0.002*peopl + 0.002*year + 0.002*chang + 0.002*gore + 0.002*al + 0.002*like + 0.002*al_gore + 0.002*go + 0.002*world + 0.002*never + 0.001*think + 0.001*one + 0.001*back + 0.001*get + 0.001*global + 0.001*us'),\n",
       " (8,\n",
       "  '0.002*climat + 0.002*one + 0.002*world + 0.002*peopl + 0.002*scienc + 0.002*know + 0.002*dr + 0.002*say + 0.002*thank + 0.002*chang + 0.001*would + 0.001*â€™ + 0.001*like + 0.001*peterson + 0.001*us + 0.001*need + 0.001*year + 0.001*great + 0.001*â€œ + 0.001*get'),\n",
       " (9,\n",
       "  '0.003*climat + 0.002*co2 + 0.002*temperatur + 0.002*chang + 0.002*â€™ + 0.002*year + 0.002*like + 0.002*thank + 0.002*dr + 0.001*climat_chang + 0.001*would + 0.001*think + 0.001*time + 0.001*peopl + 0.001*human + 0.001*see + 0.001*jordan + 0.001*level + 0.001*one + 0.001*lindzen')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating HDP Model\n",
    "hdp_model = HdpModel(corpus=corpus, id2word=id2word)\n",
    "hdp_model.show_topics()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"toni\" + 0.010*\"thank\" + 0.009*\"climat\" + 0.008*\"year\" + 0.007*\"peopl\" + 0.007*\"chang\" + 0.005*\"like\" + 0.005*\"climat_chang\" + 0.004*\"thank_toni\" + 0.004*\"â€™\"'),\n",
       " (1,\n",
       "  '0.008*\"climat\" + 0.008*\"â€™\" + 0.008*\"peopl\" + 0.007*\"ðŸ˜‚\" + 0.006*\"know\" + 0.006*\"like\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"go\" + 0.004*\"chang\"'),\n",
       " (2,\n",
       "  '0.012*\"climat\" + 0.007*\"co2\" + 0.006*\"year\" + 0.006*\"â€™\" + 0.005*\"chang\" + 0.005*\"earth\" + 0.005*\"like\" + 0.004*\"carbon\" + 0.004*\"climat_chang\" + 0.004*\"would\"'),\n",
       " (3,\n",
       "  '0.018*\"climat\" + 0.012*\"chang\" + 0.008*\"climat_chang\" + 0.007*\"â€™\" + 0.006*\"scienc\" + 0.005*\"peopl\" + 0.005*\"scientist\" + 0.004*\"one\" + 0.004*\"like\" + 0.004*\"know\"'),\n",
       " (4,\n",
       "  '0.023*\"climat\" + 0.013*\"chang\" + 0.010*\"climat_chang\" + 0.007*\"peopl\" + 0.006*\"â€™\" + 0.005*\"earth\" + 0.005*\"one\" + 0.005*\"greenhous\" + 0.004*\"warm\" + 0.004*\"co2\"'),\n",
       " (5,\n",
       "  '0.015*\"climat\" + 0.012*\"chang\" + 0.009*\"climat_chang\" + 0.009*\"â€™\" + 0.008*\"peopl\" + 0.006*\"like\" + 0.005*\"year\" + 0.004*\"get\" + 0.004*\"one\" + 0.004*\"human\"'),\n",
       " (6,\n",
       "  '0.010*\"â€™\" + 0.007*\"peopl\" + 0.007*\"like\" + 0.007*\"climat\" + 0.005*\"need\" + 0.005*\"us\" + 0.005*\"chang\" + 0.005*\"time\" + 0.005*\"would\" + 0.004*\"world\"'),\n",
       " (7,\n",
       "  '0.012*\"peopl\" + 0.012*\"â€™\" + 0.008*\"climat\" + 0.006*\"chang\" + 0.006*\"get\" + 0.005*\"like\" + 0.005*\"govern\" + 0.005*\"go\" + 0.004*\"year\" + 0.004*\"us\"'),\n",
       " (8,\n",
       "  '0.009*\"energi\" + 0.008*\"co2\" + 0.008*\"temperatur\" + 0.006*\"heat\" + 0.006*\"year\" + 0.005*\"thank\" + 0.004*\"solar\" + 0.004*\"would\" + 0.004*\"power\" + 0.004*\"need\"'),\n",
       " (9,\n",
       "  '0.017*\"climat\" + 0.008*\"co2\" + 0.007*\"chang\" + 0.007*\"temperatur\" + 0.006*\"year\" + 0.005*\"ice\" + 0.005*\"climat_chang\" + 0.005*\"peopl\" + 0.005*\"would\" + 0.004*\"like\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating LDA Model\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=10, id2word=id2word)\n",
    "lda_model.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

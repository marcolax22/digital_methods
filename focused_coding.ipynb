{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Coding\n",
    "___\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Libraries](#libraries)\n",
    "2. [Load Data](#load-data)\n",
    "3. [Data Preprocessing](#preprocessing-of-the-data)\n",
    "4. [Set Up of Dictionary](#building-dictionary)\n",
    "5. [Classifier](#classifier)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "All libraries which are needed to execute the code are listed here. Install the packages by using the `requirements.txt` file. \n",
    "\n",
    "The documentation can be found in the [README.md](README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19T21:22:45Z</td>\n",
       "      <td>1</td>\n",
       "      <td>The answer is if China and India don't help, it won't matter how much money the rest of the world  throws at reducing carbon footprint = complete waste of 50 TRILLION DOLLARS ü§¶‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19T00:43:40Z</td>\n",
       "      <td>2</td>\n",
       "      <td>and that guy is an expert, we're screwed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18T22:57:38Z</td>\n",
       "      <td>4</td>\n",
       "      <td>Kennedy is a gem.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id          published_at  like_count  \\\n",
       "0  uW6fi2tCnAc  2023-02-19T21:22:45Z           1   \n",
       "1  uW6fi2tCnAc  2023-02-19T00:43:40Z           2   \n",
       "2  uW6fi2tCnAc  2023-02-18T22:57:38Z           4   \n",
       "\n",
       "                                                                                                                                                                                  text  \\\n",
       "0  The answer is if China and India don't help, it won't matter how much money the rest of the world  throws at reducing carbon footprint = complete waste of 50 TRILLION DOLLARS ü§¶‚Äç‚ôÄÔ∏è   \n",
       "1                                                                                                                                             and that guy is an expert, we're screwed   \n",
       "2                                                                                                                                                                    Kennedy is a gem.   \n",
       "\n",
       "   author  \n",
       "0     0.0  \n",
       "1     1.0  \n",
       "2     2.0  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv('data/comments_final.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrr}\n",
      "\\toprule\n",
      "author & count & unique_video_id_count \\\\\n",
      "\\midrule\n",
      "17605 & 206 & 206 \\\\\n",
      "3743 & 188 & 124 \\\\\n",
      "18119 & 166 & 161 \\\\\n",
      "17604 & 155 & 138 \\\\\n",
      "18380 & 154 & 154 \\\\\n",
      "17977 & 142 & 138 \\\\\n",
      "17906 & 139 & 115 \\\\\n",
      "17676 & 135 & 103 \\\\\n",
      "17755 & 134 & 134 \\\\\n",
      "14017 & 129 & 126 \\\\\n",
      "14057 & 128 & 123 \\\\\n",
      "2610 & 125 & 125 \\\\\n",
      "25732 & 116 & 88 \\\\\n",
      "1645 & 107 & 62 \\\\\n",
      "17581 & 106 & 104 \\\\\n",
      "6165 & 103 & 52 \\\\\n",
      "6 & 99 & 98 \\\\\n",
      "1106 & 97 & 75 \\\\\n",
      "17608 & 92 & 80 \\\\\n",
      "17590 & 87 & 79 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group data by author and see distribution of comments \n",
    "df['author'] = pd.to_numeric(df['author'], errors='coerce').astype('Int64')\n",
    "summary = df.groupby('author').agg(\n",
    "    count=('author', 'size'),\n",
    "    unique_video_id_count=('video_id', 'nunique')\n",
    ").reset_index()\n",
    "summary.sort_values(by='count', ascending=False, inplace=True)\n",
    "\n",
    "# print top 20 authors\n",
    "latex_table = summary.head(20).to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract original text for seeing in comparising capitalized words etc.\n",
    "extracted_col = df[\"text\"]\n",
    "\n",
    "# process data with using functions from functions.py\n",
    "processed_df = (\n",
    "    df.pipe(remove_users, 'text')\n",
    "      .pipe(lowercase_text, 'text')\n",
    "      .pipe(remove_whitespace, 'text')\n",
    "      .pipe(remove_punctuation, 'text')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>og_text</th>\n",
       "      <th>og_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19T21:22:45Z</td>\n",
       "      <td>1</td>\n",
       "      <td>the answer is if china and india dont help it wont matter how much money the rest of the world throws at reducing carbon footprint  complete waste of 50 trillion dollars ü§¶‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The answer is if China and India don't help, it won't matter how much money the rest of the world  throws at reducing carbon footprint = complete waste of 50 TRILLION DOLLARS ü§¶‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>The answer is if China and India don't help, it won't matter how much money the rest of the world  throws at reducing carbon footprint = complete waste of 50 TRILLION DOLLARS ü§¶‚Äç‚ôÄÔ∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-19T00:43:40Z</td>\n",
       "      <td>2</td>\n",
       "      <td>and that guy is an expert were screwed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>and that guy is an expert, we're screwed</td>\n",
       "      <td>and that guy is an expert, we're screwed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uW6fi2tCnAc</td>\n",
       "      <td>2023-02-18T22:57:38Z</td>\n",
       "      <td>4</td>\n",
       "      <td>kennedy is a gem</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Kennedy is a gem.</td>\n",
       "      <td>Kennedy is a gem.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id          published_at  like_count  \\\n",
       "0  uW6fi2tCnAc  2023-02-19T21:22:45Z           1   \n",
       "1  uW6fi2tCnAc  2023-02-19T00:43:40Z           2   \n",
       "2  uW6fi2tCnAc  2023-02-18T22:57:38Z           4   \n",
       "\n",
       "                                                                                                                                                                             text  \\\n",
       "0  the answer is if china and india dont help it wont matter how much money the rest of the world throws at reducing carbon footprint  complete waste of 50 trillion dollars ü§¶‚Äç‚ôÄÔ∏è   \n",
       "1                                                                                                                                          and that guy is an expert were screwed   \n",
       "2                                                                                                                                                                kennedy is a gem   \n",
       "\n",
       "   author  \\\n",
       "0     0.0   \n",
       "1     1.0   \n",
       "2     2.0   \n",
       "\n",
       "                                                                                                                                                                               og_text  \\\n",
       "0  The answer is if China and India don't help, it won't matter how much money the rest of the world  throws at reducing carbon footprint = complete waste of 50 TRILLION DOLLARS ü§¶‚Äç‚ôÄÔ∏è   \n",
       "1                                                                                                                                             and that guy is an expert, we're screwed   \n",
       "2                                                                                                                                                                    Kennedy is a gem.   \n",
       "\n",
       "                                                                                                                                                                               og_text  \n",
       "0  The answer is if China and India don't help, it won't matter how much money the rest of the world  throws at reducing carbon footprint = complete waste of 50 TRILLION DOLLARS ü§¶‚Äç‚ôÄÔ∏è  \n",
       "1                                                                                                                                             and that guy is an expert, we're screwed  \n",
       "2                                                                                                                                                                    Kennedy is a gem.  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the extracted column to the second DataFrame\n",
    "processed_df = pd.concat([processed_df, extracted_col.rename(\"og_text\")], axis=1)\n",
    "processed_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lemmatization to reduce words to their root form\n",
    "processed_df['text'] = processed_df['text'].astype('str')\n",
    "processed_df = lemmatize_words(processed_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.lemmatized_text = processed_df.lemmatized_text.apply(lambda x: '' if str(x) == 'nan' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "substrings = ['god plan', 'greenhouse gas', 'natural cycle', 'hoax']\n",
    "string = ['wef']\n",
    "pattern = '|'.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>og_text</th>\n",
       "      <th>og_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90961</th>\n",
       "      <td>ThTkXT06UiM</td>\n",
       "      <td>2024-01-19T04:02:32Z</td>\n",
       "      <td>1</td>\n",
       "      <td>the narrative is indoctrination based on mainstream media and weird science  the wef who are globalism  the oceans are rich with volcanic activity as well water warming has little to do with cow farts and carbon units war and the machines of war seem to be acceptable though peel back the economic foreskin and expose the 12 inch of reality üåàüêõüå™</td>\n",
       "      <td>@williamrome2257</td>\n",
       "      <td>the narrative is indoctrination based on mainstream media and weird science  the wef who are globalism  the oceans are rich with volcanic activity as well water warming has little to do with cow farts and carbon units war and the machines of war seem to be acceptable though peel back the economic foreskin and expose the 12 inch of reality üåàüêõüå™</td>\n",
       "      <td>The narrative is indoctrination. Based on  mainstream media and weird science . The WEF WHO,  are Globalism  .     The Oceans are rich with volcanic activity as well. Water warming. has little to do with cow farts and carbon units.  War and the machines of war seem to be acceptable though.  Peel back the Economic Foreskin and expose the 1/2 inch of reality.   üåàüêõüå™</td>\n",
       "      <td>the narrative be indoctrination base on mainstream medium and weird science the wef who be globalism the ocean be rich with volcanic activity as well water warm have little to do with cow fart and carbon unit war and the machine of war seem to be acceptable though peel back the economic foreskin and expose the 12 inch of reality üåàüêõüå™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>ry-bRYhN1Xs</td>\n",
       "      <td>2023-02-02T18:00:56Z</td>\n",
       "      <td>0</td>\n",
       "      <td>cut the budget starting here hanging witj the wef is a clue these people are pure evil</td>\n",
       "      <td>@chuckbabbs2726</td>\n",
       "      <td>cut the budget starting here hanging witj the wef is a clue these people are pure evil</td>\n",
       "      <td>Cut the budget starting here, hanging witj the WEF is a clue these people are pure evil!</td>\n",
       "      <td>cut the budget starting here hang witj the wef be a clue these people be pure evil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21241</th>\n",
       "      <td>reaABJ5HpLk</td>\n",
       "      <td>2023-01-16T20:54:20Z</td>\n",
       "      <td>0</td>\n",
       "      <td>hey doc in a weird turn it seems to me that organizations like the wef the un and others have taken the malthuthian theory and twisted it old enough to remember the 70s and the miniice age then al gore peace be upon him and how millions of people would perish by 2016 stupid</td>\n",
       "      <td>@mikedawson1376</td>\n",
       "      <td>hey doc in a weird turn it seems to me that organizations like the wef the un and others have taken the malthuthian theory and twisted it old enough to remember the 70s and the miniice age then al gore peace be upon him and how millions of people would perish by 2016 stupid</td>\n",
       "      <td>Hey doc. In a weird turn, it seems to me that organizations like the WEF, the UN and others have taken the Malthuthian theory and twisted it. Old enough to remember the '70s and the mini-ice age, then Al Gore (peace be upon him) and how millions of people would perish by 2016. Stupid.</td>\n",
       "      <td>hey doc in a weird turn it seem to me that organization like the wef the un and others have take the malthuthian theory and twist it old enough to remember the 70 and the miniice age then al gore peace be upon him and how million of people would perish by 2016 stupid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id          published_at  like_count  \\\n",
       "90961  ThTkXT06UiM  2024-01-19T04:02:32Z           1   \n",
       "2609   ry-bRYhN1Xs  2023-02-02T18:00:56Z           0   \n",
       "21241  reaABJ5HpLk  2023-01-16T20:54:20Z           0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "90961  the narrative is indoctrination based on mainstream media and weird science  the wef who are globalism  the oceans are rich with volcanic activity as well water warming has little to do with cow farts and carbon units war and the machines of war seem to be acceptable though peel back the economic foreskin and expose the 12 inch of reality üåàüêõüå™   \n",
       "2609                                                                                                                                                                                                                                                                     cut the budget starting here hanging witj the wef is a clue these people are pure evil   \n",
       "21241                                                                        hey doc in a weird turn it seems to me that organizations like the wef the un and others have taken the malthuthian theory and twisted it old enough to remember the 70s and the miniice age then al gore peace be upon him and how millions of people would perish by 2016 stupid   \n",
       "\n",
       "                 author  \\\n",
       "90961  @williamrome2257   \n",
       "2609    @chuckbabbs2726   \n",
       "21241   @mikedawson1376   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                        og_text  \\\n",
       "90961  the narrative is indoctrination based on mainstream media and weird science  the wef who are globalism  the oceans are rich with volcanic activity as well water warming has little to do with cow farts and carbon units war and the machines of war seem to be acceptable though peel back the economic foreskin and expose the 12 inch of reality üåàüêõüå™   \n",
       "2609                                                                                                                                                                                                                                                                     cut the budget starting here hanging witj the wef is a clue these people are pure evil   \n",
       "21241                                                                        hey doc in a weird turn it seems to me that organizations like the wef the un and others have taken the malthuthian theory and twisted it old enough to remember the 70s and the miniice age then al gore peace be upon him and how millions of people would perish by 2016 stupid   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                             og_text  \\\n",
       "90961  The narrative is indoctrination. Based on  mainstream media and weird science . The WEF WHO,  are Globalism  .     The Oceans are rich with volcanic activity as well. Water warming. has little to do with cow farts and carbon units.  War and the machines of war seem to be acceptable though.  Peel back the Economic Foreskin and expose the 1/2 inch of reality.   üåàüêõüå™   \n",
       "2609                                                                                                                                                                                                                                                                                        Cut the budget starting here, hanging witj the WEF is a clue these people are pure evil!   \n",
       "21241                                                                                  Hey doc. In a weird turn, it seems to me that organizations like the WEF, the UN and others have taken the Malthuthian theory and twisted it. Old enough to remember the '70s and the mini-ice age, then Al Gore (peace be upon him) and how millions of people would perish by 2016. Stupid.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                      lemmatized_text  \n",
       "90961  the narrative be indoctrination base on mainstream medium and weird science the wef who be globalism the ocean be rich with volcanic activity as well water warm have little to do with cow fart and carbon unit war and the machine of war seem to be acceptable though peel back the economic foreskin and expose the 12 inch of reality üåàüêõüå™  \n",
       "2609                                                                                                                                                                                                                                                               cut the budget starting here hang witj the wef be a clue these people be pure evil  \n",
       "21241                                                                     hey doc in a weird turn it seem to me that organization like the wef the un and others have take the malthuthian theory and twist it old enough to remember the 70 and the miniice age then al gore peace be upon him and how million of people would perish by 2016 stupid  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = processed_df[processed_df['lemmatized_text'].str.contains(pattern, case=False, na=False)]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(len(filtered_df))\n",
    "filtered_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96595/96595 [00:05<00:00, 16631.60it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96595/96595 [00:00<00:00, 160833.26it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tqdm.pandas() #Creates a progress bar and below use \"progress_apply\" instead of \"apply\" to create a progress bar (This is more of a \"nice to have\" than a \"need to have\")\n",
    "\n",
    "#Tokenizing and creating a column of unigrams from the stemmed tweet text. \n",
    "df['unigrams'] = df['stemmed_text'].progress_apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "#Defining a function that will create bigrams \n",
    "def bigrams(doc): # a doc is a list of unigrams in same order as in tweets \n",
    "    \n",
    "    bigrams = [] #Empty list to save the bigrams\n",
    "    \n",
    "    for bigram in list(nltk.bigrams(doc)):  #Creating bigrams as tuples with nltk.bigrams and iterating over these them\n",
    "        bigrams.append(\"_\".join(bigram))    #Joining each bigram-tuple pair with an underscore and saving to list\n",
    "    \n",
    "    return bigrams\n",
    "\n",
    "#Creating a column with bigrams by applying function to column of unigrams\n",
    "df['bigrams'] = df.unigrams.progress_apply(lambda x: bigrams(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['lemmatized_text'].str.contains('natural cycle', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            all_n_grams_lemmatized        category_lemmatized_comments\n",
      "0  [weather, climate, temperature]  [climate_related, weather_related]\n",
      "1                [climate, change]                   [climate_related]\n",
      "2                 [rain, humidity]                   [weather_related]\n",
      "3                       [sun, sky]                   [weather_related]\n",
      "Number of rows containing 'climate': 2\n"
     ]
    }
   ],
   "source": [
    "def classify_comments(comments, keyword_dict):\n",
    "    classifications = []  # Initialize an empty list of classifications\n",
    "    \n",
    "    for comment in comments:  # Loop through each comment in the DataFrame\n",
    "        categories = []  # Initialize an empty list of categories\n",
    "        \n",
    "        for category, keywords in keyword_dict.items():  # Iterate through each key-value pair in the dictionary\n",
    "            for keyword in keywords:  # For each category, iterate through the list of keywords\n",
    "                if keyword in comment:  # Check if the keyword is in the list directly\n",
    "                    categories.append(category)  # If a keyword is found, the category is appended to the list\n",
    "                    break  # Stop checking more keywords for this category\n",
    "        \n",
    "        if not categories:\n",
    "            categories = ['uncategorized']  # If no keywords are found, mark as 'uncategorized'\n",
    "        \n",
    "        classifications.append(categories)\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "# Example DataFrame and keyword dictionary\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'all_n_grams_lemmatized': [\n",
    "        ['weather', 'climate', 'temperature'],\n",
    "        ['climate', 'change'],\n",
    "        ['rain', 'humidity'],\n",
    "        ['sun', 'sky']\n",
    "    ]\n",
    "}\n",
    "\n",
    "keyword_dict_lemmatized = {\n",
    "    'climate_related': ['climate', 'temperature'],\n",
    "    'weather_related': ['weather', 'rain', 'humidity', 'sun', 'sky']\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "processed_df = pd.DataFrame(data)\n",
    "\n",
    "# Apply classifier to lemmatized comments \n",
    "processed_df['category_lemmatized_comments'] = classify_comments(processed_df['all_n_grams_lemmatized'], keyword_dict_lemmatized)\n",
    "\n",
    "# Output the DataFrame to check results\n",
    "print(processed_df.head())\n",
    "\n",
    "# Verify the number of rows containing 'climate' matches the previous function\n",
    "climate_mask = processed_df['all_n_grams_lemmatized'].apply(lambda x: 'climate' in x)\n",
    "sum_rows_with_climate = climate_mask.sum()\n",
    "\n",
    "print(f\"Number of rows containing 'climate': {sum_rows_with_climate}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
